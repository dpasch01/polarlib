{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2556380",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './polar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/v3/infodemic/resources/global/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904f6a3",
   "metadata": {},
   "source": [
    "### Calculate Sentiment Attitude\n",
    "\n",
    "For each pair of entities and entity-np, calculate the sentiment attitude from the one towards the other. This is done by looking at the sentiment score of the dependency path between the two. To calculate the sentiment score, we utilize the debater lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mpqa_df = []\n",
    "\n",
    "with open('./resources/mpqa/subjclueslen1-HLTEMNLP05.tff', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        obj = {}\n",
    "        \n",
    "        for d in l.strip().split(' '):\n",
    "            d = d.split('=')\n",
    "            obj[d[0]] = d[1] if d[0] != 'len' else int(d[1])\n",
    "            \n",
    "        mpqa_df.append(obj)\n",
    "        \n",
    "mpqa_df = pd.DataFrame.from_dict(mpqa_df).set_index('word1')\n",
    "\n",
    "mpqa_dict = mpqa_df.T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b753db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def calculate_mpqa(tokens):\n",
    "    \n",
    "    positive_list, negative_list = [], []\n",
    "    positive_words, negative_words = [], []\n",
    "    \n",
    "    for token in tokens:\n",
    "        sentiment = 0\n",
    "        if 'entity_id' in token: continue\n",
    "            \n",
    "        t = token['originalText'].lower().strip()\n",
    "        if t in stop_words: continue\n",
    "        if not t in list(mpqa_dict.keys()): t = token['lemma'].lower().strip()\n",
    "        if not t in list(mpqa_dict.keys()): continue\n",
    "        if t in stop_words: continue\n",
    "            \n",
    "        mpqa_obj = mpqa_dict[t]\n",
    "\n",
    "        t_pos = convert_to_mpqa_pos(token['pos'])\n",
    "        \n",
    "        if not (mpqa_obj['pos1'] == 'anypos' or t_pos == mpqa_obj['pos1']): continue\n",
    "        \n",
    "        mpqa_polarity = mpqa_obj['priorpolarity']\n",
    "\n",
    "        if mpqa_polarity == 'positive' or mpqa_polarity == 'both': \n",
    "            positive_words.append(t)\n",
    "            positive_list.append(1.0)\n",
    "            \n",
    "        if mpqa_polarity == 'negative' or mpqa_polarity == 'both': \n",
    "            negative_words.append(t)\n",
    "            negative_list.append(1.0)\n",
    "    \n",
    "    return {'POSITIVE': sum(positive_list), 'NEGATIVE': abs(sum(negative_list))}, \\\n",
    "           {'POSITIVE': positive_words, 'NEGATIVE': negative_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906edbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del stop_words[37] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72454a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1452cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mpqa_pos(pos):\n",
    "    if    pos == 'VERB': return 'verb'\n",
    "    elif  pos == 'NOUN' or pos == 'PROPN': return 'noun'\n",
    "    elif  pos == 'ADJ': return 'adj'\n",
    "    elif  pos == 'ADV': return 'adverb'\n",
    "    else: return 'other'\n",
    "    \n",
    "def calculate_sentiment_attitude(path):\n",
    "\n",
    "    daily_folder = path.split('/')[-2]\n",
    "\n",
    "    with open(path, 'r') as f: pair_annotation_dependency_features = json.load(f)    \n",
    "    pair_annotation_dependency_features['dependency_features'] = jsonpickle.decode(pair_annotation_dependency_features['dependency_features'])\n",
    "\n",
    "    uid = pair_annotation_dependency_features['uid']\n",
    "    pair_annotation_dependency_features = pair_annotation_dependency_features['dependency_features']\n",
    "\n",
    "    pair_sentiment_attitude_dict = defaultdict(lambda: {'POSITIVE': [], 'NEGATIVE': []})\n",
    "    pair_sentiment_word_dict = defaultdict(lambda: {'POSITIVE': [], 'NEGATIVE': []})\n",
    "\n",
    "    for pair in pair_annotation_dependency_features:\n",
    "        for att_obj in pair_annotation_dependency_features[pair]:\n",
    "            \n",
    "            sentiment_attitudes, sentiment_words = calculate_mpqa(att_obj)\n",
    "\n",
    "            pair_sentiment_attitude_dict[pair]['POSITIVE'].append(sentiment_attitudes['POSITIVE'])\n",
    "            pair_sentiment_attitude_dict[pair]['NEGATIVE'].append(sentiment_attitudes['NEGATIVE'])\n",
    "\n",
    "            pair_sentiment_word_dict[pair]['POSITIVE'] += sentiment_words['POSITIVE']\n",
    "            pair_sentiment_word_dict[pair]['NEGATIVE'] += sentiment_words['NEGATIVE']\n",
    "\n",
    "    pair_sentiment_attitude_dict = dict(pair_sentiment_attitude_dict)\n",
    "    pair_sentiment_word_dict = dict(pair_sentiment_word_dict)\n",
    "\n",
    "    output_folder = output_dir + 'pair_sentiment_attitudes/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'sentiment_attitudes': jsonpickle.encode(pair_sentiment_attitude_dict)\n",
    "    }))\n",
    "\n",
    "    output_folder = output_dir + 'pair_sentiment_words/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'sentiment_words': jsonpickle.encode(pair_sentiment_word_dict)\n",
    "    }))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "\n",
    "pair_paths = [output_dir + 'pair_dependency_features/' + p + '/' for p in sorted(os.listdir(output_dir + 'pair_dependency_features/'))]\n",
    "pair_paths = list(itertools.chain.from_iterable([[p + d for d in os.listdir(p)] for p in pair_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77feeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "import multiprocessing, json, pickle, jsonpickle\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count())\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(calculate_sentiment_attitude, pair_paths),\n",
    "    desc='Calculating entity-pair sentiment attitudes',\n",
    "    total=len(pair_paths)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368acb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "\n",
    "#######################################\n",
    "# Define a loading function for the   #\n",
    "# .gzip files we generate. Examples:  #\n",
    "# =================================== #\n",
    "# for .json.gzip use func=json.loads  #\n",
    "# for .pckl.gzip use func=pickle.load #\n",
    "#######################################\n",
    "\n",
    "def load_gzip(path, func=json.loads):\n",
    "    with gzip.open(path, 'r') as f: data = func(f.read().decode('utf-8'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e40c7c",
   "metadata": {},
   "source": [
    "### Export `Entity-NP Sentiment Attitudes`\n",
    "\n",
    "Export the attitude objects with entities as sources and NPs as targets. This will later help identify the polarizing topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ee570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sentiment_features, jsonpickle\n",
    "from utilities import DepDirection, find_dep_path\n",
    "\n",
    "def calculate_entity_np_attitudes(path):\n",
    "\n",
    "    uid = path.split('/')[-1]\n",
    "    daily_folder = path.split('/')[-2]\n",
    "\n",
    "    path = path.replace('/spacy/', '/{}/').replace('.pckl', '{}')\n",
    "    \n",
    "    if not os.path.exists(path.format('entity_np_annotations', '.json')): return None\n",
    "\n",
    "    try:\n",
    "        entity_np_annotations_dict = load_gzip(path.format('entity_np_annotations', '.json'))    \n",
    "    except Exception as ex:\n",
    "        print(daily_folder, uid, ex)\n",
    "        return None\n",
    "    \n",
    "    entity_np_annotations_dict = entity_np_annotations_dict['entity_np_annotations']\n",
    "\n",
    "    entity_np_sentiment_attitudes = defaultdict(lambda: defaultdict(lambda: {'POSITIVE': [], 'NEGATIVE': []}))\n",
    "    entity_np_sentiment_word_dict = defaultdict(lambda: defaultdict(lambda: {'POSITIVE': [], 'NEGATIVE': []}))\n",
    "\n",
    "    for entity in entity_np_annotations_dict:\n",
    "\n",
    "        for np in entity_np_annotations_dict[entity]:\n",
    "            np_dep_path_feature = []\n",
    "\n",
    "            for annotation in entity_np_annotations_dict[entity][np]:\n",
    "                tokens = annotation['tokens']\n",
    "                source_indices = [i for i, t in enumerate(tokens) if 'entity_id' in t and t['entity_id'][1] == entity]\n",
    "                destination_indices = [i for i, t in enumerate(tokens) if 'entity_id' in t and t['entity_id'][1] == np]\n",
    "\n",
    "                for source_destination in itertools.product(source_indices, destination_indices):\n",
    "                    dep_path = find_dep_path(tokens, source_destination[0], source_destination[1])\n",
    "\n",
    "                    dep_path = [\n",
    "                        (( DepDirection.DEP if dep_dir == DepDirection.GOV else DepDirection.GOV, dep_type), dep_idx) \n",
    "                        for (dep_dir, dep_type), dep_idx in dep_path\n",
    "                    ]\n",
    "\n",
    "                    dep_path_features = sentiment_features.dep_path_features([], tokens, dep_path)\n",
    "\n",
    "                    if len(dep_path_features) > 0: np_dep_path_feature += [dpf[1] for dpf in dep_path_features]\n",
    "\n",
    "                    dep_path = find_dep_path(tokens, source_destination[1], source_destination[0])\n",
    "\n",
    "                    dep_path = [\n",
    "                        (( DepDirection.DEP if dep_dir == DepDirection.GOV else DepDirection.GOV, dep_type), dep_idx) \n",
    "                        for (dep_dir, dep_type), dep_idx in dep_path\n",
    "                    ]\n",
    "\n",
    "                    dep_path_features = sentiment_features.dep_path_features([], tokens, dep_path)\n",
    "\n",
    "                    if len(dep_path_features) > 0: np_dep_path_feature += [dpf[1] for dpf in dep_path_features]\n",
    "\n",
    "            for att_obj in np_dep_path_feature:\n",
    "                sentiment_attitudes, sentiment_words = calculate_mpqa(att_obj)\n",
    "\n",
    "                entity_np_sentiment_attitudes[entity][np]['POSITIVE'].append(sentiment_attitudes['POSITIVE'])\n",
    "                entity_np_sentiment_attitudes[entity][np]['NEGATIVE'].append(sentiment_attitudes['NEGATIVE'])\n",
    "\n",
    "                entity_np_sentiment_word_dict[entity][np]['POSITIVE'] += sentiment_words['POSITIVE']\n",
    "                entity_np_sentiment_word_dict[entity][np]['NEGATIVE'] += sentiment_words['NEGATIVE']\n",
    "\n",
    "    output_folder = output_dir + 'entity_np_sentiment_attitudes/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.jsonpckl'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'entity_np_sentiment_attitudes': jsonpickle.encode(entity_np_sentiment_attitudes)\n",
    "    }))\n",
    "\n",
    "    output_folder = output_dir + 'entity_np_sentiment_words/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.jsonpckl'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'entity_np_sentiment_words': jsonpickle.encode(entity_np_sentiment_word_dict)\n",
    "    })) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65ad32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77746df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools, json\n",
    "\n",
    "spacy_paths = [output_dir + 'spacy/' + p + '/' for p in sorted(os.listdir(output_dir + 'spacy/'))]\n",
    "spacy_paths = list(itertools.chain.from_iterable([[p + _ for _ in os.listdir(p)] for p in spacy_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pool = Pool(32)\n",
    "\n",
    "for d in tqdm(\n",
    "    pool.imap_unordered(calculate_entity_np_attitudes, spacy_paths),\n",
    "    desc='Calculating entity-np attitudes',\n",
    "    total=len(spacy_paths)\n",
    "): del d\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0521ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, jsonpickle, numpy\n",
    "from collections import defaultdict\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "def calculate_pair_frequencies(path):\n",
    "    with open(path, 'r') as f: pair_sentiment_attitude_dict = json.load(f)    \n",
    "    pair_sentiment_attitude_dict['sentiment_attitudes'] = jsonpickle.decode(pair_sentiment_attitude_dict['sentiment_attitudes'])\n",
    "    pair_sentiment_attitude_dict = pair_sentiment_attitude_dict['sentiment_attitudes']\n",
    "    \n",
    "    return [{k: v} for k, v in pair_sentiment_attitude_dict.items()]\n",
    "\n",
    "def calculate_pair_words(path):\n",
    "    with open(path, 'r') as f: pair_sentiment_attitude_dict = json.load(f)    \n",
    "    pair_sentiment_attitude_dict['sentiment_words'] = jsonpickle.decode(pair_sentiment_attitude_dict['sentiment_words'])\n",
    "    pair_sentiment_attitude_dict = pair_sentiment_attitude_dict['sentiment_words']\n",
    "    \n",
    "    return [{k: v} for k, v in pair_sentiment_attitude_dict.items() if len(v['POSITIVE']) > 0 or len(v['NEGATIVE']) > 0]\n",
    "\n",
    "def sentiment_threshold_difference(swn_pos, swn_neg):\n",
    "    swn_pos = abs(swn_pos)\n",
    "    swn_neg = abs(swn_neg)\n",
    "    return numpy.sign(swn_pos - swn_neg) * (abs(swn_pos - swn_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "diaily_article_dict = {}\n",
    "\n",
    "for str_date in os.listdir(output_dir + 'pair_sentiment_attitudes'):\n",
    "    diaily_article_dict[str_date] = os.listdir(output_dir + 'pair_sentiment_attitudes/' + str_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "diaily_article_freq = {datetime.datetime.strptime(k, '%Y%m%d'): len(v) for k,v in diaily_article_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41981b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = sorted(diaily_article_freq.keys(), key=lambda k: k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.bar(\n",
    "    x=date_list,\n",
    "    height=[diaily_article_freq[d] for d in date_list]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcef6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx, itertools\n",
    "\n",
    "window_articles = list(itertools.chain.from_iterable(diaily_article_dict.values()))\n",
    "\n",
    "attitude_paths = [output_dir + f'/pair_sentiment_attitudes/{d}/{p}' for p in window_articles]\n",
    "attitude_paths = [p for p in attitude_paths if os.path.exists(p)]\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count() - 4)\n",
    "\n",
    "_pair_sentiment_attitude_dict = []\n",
    "\n",
    "for result in tqdm(\n",
    "    pool.imap_unordered(calculate_pair_frequencies, attitude_paths),\n",
    "    desc='Fetching Pairs',\n",
    "    total=len(attitude_paths)\n",
    "): _pair_sentiment_attitude_dict += result\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_sentiment_attitude_dict = {}\n",
    "\n",
    "for att_obj in tqdm(_pair_sentiment_attitude_dict):\n",
    "\n",
    "    pair, atts = list(att_obj.items())[0]\n",
    "\n",
    "    pair = make_tuple(pair)\n",
    "    pair = list(pair)\n",
    "\n",
    "    pair[0] = pair[0].replace('http://wat.org/resource/', 'http://dbpedia.org/resource/')\n",
    "    pair[1] = pair[1].replace('http://wat.org/resource/', 'http://dbpedia.org/resource/')\n",
    "\n",
    "    if pair[0] == pair[1]: continue\n",
    "\n",
    "    pair.sort()\n",
    "\n",
    "    pair = (pair[0], pair[1])\n",
    "    \n",
    "    if pair not in pair_sentiment_attitude_dict: pair_sentiment_attitude_dict[pair] = {'POSITIVE': [], 'NEGATIVE': []}\n",
    "\n",
    "    pair_sentiment_attitude_dict[pair]['POSITIVE'] += atts['POSITIVE']\n",
    "    pair_sentiment_attitude_dict[pair]['NEGATIVE'] += atts['NEGATIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3709109",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pair_frequency_dict = {}\n",
    "\n",
    "for p in tqdm(pair_sentiment_attitude_dict):\n",
    "   \n",
    "    sentiments = []\n",
    "\n",
    "    for i in range(len(pair_sentiment_attitude_dict[p]['POSITIVE'])):\n",
    "\n",
    "        total_p_n = pair_sentiment_attitude_dict[p]['POSITIVE'][i] + pair_sentiment_attitude_dict[p]['NEGATIVE'][i]\n",
    "        \n",
    "        if total_p_n == 0.0: sentiments.append(0.0)\n",
    "        else: sentiments.append(\n",
    "            sentiment_threshold_difference(\n",
    "                pair_sentiment_attitude_dict[p]['POSITIVE'][i] / total_p_n,\n",
    "                pair_sentiment_attitude_dict[p]['NEGATIVE'][i] / total_p_n\n",
    "            )\n",
    "        )\n",
    "\n",
    "    entity_pair_frequency_dict[p] = len([s for s in sentiments if s != 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bfa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "numpy.percentile(sorted(list(set(entity_pair_frequency_dict.values()))), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a6bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c54027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "node_id, node_to_int, int_to_node = 0, {}, {}\n",
    "\n",
    "for p in tqdm(sorted(entity_pair_frequency_dict.keys(), key = lambda k: entity_pair_frequency_dict[k], reverse = True)):\n",
    "    \n",
    "    p_freq = entity_pair_frequency_dict[p]\n",
    "    if p_freq < 2: continue\n",
    "    \n",
    "    n_v = numpy.asarray([v for v in pair_sentiment_attitude_dict[p]['NEGATIVE']])\n",
    "    p_v = numpy.asarray([v for v in pair_sentiment_attitude_dict[p]['POSITIVE']])\n",
    "    \n",
    "    sentiments = []\n",
    "\n",
    "    for j in range(p_v.shape[0]): \n",
    "        p_n_total = p_v[j] + n_v[j]\n",
    "        \n",
    "        if p_n_total == 0: sentiments.append(0.0)\n",
    "        else: sentiments.append(sentiment_threshold_difference(p_v[j] / p_n_total, n_v[j] / p_n_total))\n",
    "        \n",
    "    sentiments = [s for s in sentiments if s != 0]    \n",
    "    \n",
    "    if len(sentiments) == 0: continue\n",
    "        \n",
    "    sentiment = numpy.median(sentiments)\n",
    "        \n",
    "    if sentiment < 0.01 and sentiment > -0.01: continue\n",
    "    \n",
    "    if not p[0] in node_to_int: \n",
    "        node_to_int[p[0]] = node_id\n",
    "        int_to_node[node_id] = p[0]\n",
    "        node_id += 1\n",
    "\n",
    "    if not p[1] in node_to_int: \n",
    "        node_to_int[p[1]] = node_id\n",
    "        int_to_node[node_id] = p[1]\n",
    "        node_id += 1\n",
    "\n",
    "    p_1, p_2 = node_to_int[p[0]], node_to_int[p[1]]\n",
    "        \n",
    "    G.add_edge(p_1, p_2, weight=numpy.sign(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nodes:', G.number_of_nodes())\n",
    "print('Edges:', G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_freq_dict = {}\n",
    "\n",
    "for n in tqdm(G.nodes()):\n",
    "    n1 = int_to_node[n]\n",
    "    \n",
    "    f1 = 0.0\n",
    "    \n",
    "    for n2 in G.neighbors(n):\n",
    "        n2  = int_to_node[n2]\n",
    "        \n",
    "        p = [n1, n2]\n",
    "        p.sort()\n",
    "        p = (p[0], p[1])\n",
    "        \n",
    "        f12 = entity_pair_frequency_dict[p]\n",
    "        f1 += f12\n",
    "        \n",
    "    node_freq_dict[n1] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4335f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "for n, f in sorted(node_freq_dict.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    print('{0:5} {1:85} {2}'.format(index, n.replace('http://dbpedia.org/resource/', ''), f))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "\n",
    "if os.path.exists(output_dir + 'pkb/'): print('File already exists.')\n",
    "else: os.makedirs(output_dir + 'pkb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'pkb/' + 'sag.pckl', 'wb') as f:         pickle.dump(G, f)\n",
    "with open(output_dir + 'pkb/' + 'int_to_node.pckl', 'wb') as f: pickle.dump(int_to_node, f)\n",
    "with open(output_dir + 'pkb/' + 'node_to_int.pckl', 'wb') as f: pickle.dump(node_to_int, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c63919",
   "metadata": {},
   "source": [
    "### Use `SiMap` to Extract Fellowships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ac169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, pandas as pd\n",
    "from signed_network_balance import *\n",
    "\n",
    "def si_map_cpm(G, resolution=0.00):\n",
    "    if os.path.isfile('/tmp/simap.wrapper.tsv'): print('Removing simap previous data...', os.remove('/tmp/simap.wrapper.tsv'))\n",
    "    if os.path.isfile('/tmp/simap.wrapper.partition.out'): print('Removing simap previous partitions...', os.remove('/tmp/simap.wrapper.partition.out'))\n",
    "        \n",
    "    _df_dict = [{'p_1': max(e[0], e[1]), 'p_2': min(e[1], e[0]), 'sign': int(e[2]['weight'])} for e in list(G.edges(data=True))]\n",
    "\n",
    "    _df = pd.DataFrame.from_dict(_df_dict)\n",
    "    _df = _df.sort_values(by=['p_1'])\n",
    "    \n",
    "    print('> Dumping graph in /tmp/simap.wrapper.tsv...')\n",
    "    _df.to_csv('/tmp/simap.wrapper.tsv', sep='\\t', index=False, header=False)\n",
    "    \n",
    "    subprocess_results = subprocess.run(\n",
    "        ['java', '-jar', './simap-1.0.0-final.jar', 'mdl', '-r', str(resolution), '-g', '/tmp/simap.wrapper.tsv', '-o', '/tmp/simap.wrapper.partition.out'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    print('> Errors: ', str(subprocess_results.stderr))\n",
    "    print('> Outputs: ', str(subprocess_results.stdout))\n",
    "    \n",
    "    _df_partitions = pd.read_csv('/tmp/simap.wrapper.partition.out', sep='\\t', index_col=0, header=None)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return {k: v[1] for k,v in _df_partitions.T.to_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simap_iteration_dict = {i: [] for i in range(10)}\n",
    "\n",
    "for iteration in range(10):\n",
    "    \n",
    "    si_map_0 = si_map_cpm(G.copy(), resolution=0.075)\n",
    "    \n",
    "    si_map_partitions = defaultdict(lambda: [])\n",
    "\n",
    "    for k,v in si_map_0.items(): si_map_partitions[v].append(k)\n",
    "    for k in list(si_map_partitions.keys()): si_map_partitions[k] = list(si_map_partitions[k])\n",
    "\n",
    "    si_map_partitions = dict(si_map_partitions)\n",
    "    \n",
    "    for i in range(len(si_map_partitions)):\n",
    "        f_list = []\n",
    "        \n",
    "        for n in si_map_partitions[i]: f_list.append(int_to_node[n])\n",
    "\n",
    "        simap_iteration_dict[iteration].append(f_list.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b7301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(s1, s2): return len(set(s1).intersection(set(s2))) / len(set(s1).union(set(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902348cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowship_indices = [['{}_{}'.format(i, j) for j, f in enumerate(f_list)] for i, f_list in simap_iteration_dict.items()]\n",
    "fellowship_indices = list(itertools.chain.from_iterable(fellowship_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_indices = []\n",
    "\n",
    "for i, f1 in tqdm(list(enumerate(fellowship_indices))):\n",
    "    x1 = int(f1.split('_')[0])\n",
    "    y1 = int(f1.split('_')[1])\n",
    "    \n",
    "    j_f12 = []\n",
    "    \n",
    "    for j, f2 in enumerate(fellowship_indices):\n",
    "        x2 = int(f2.split('_')[0])\n",
    "        y2 = int(f2.split('_')[1])\n",
    "        \n",
    "        d12 = 1.0 - jaccard_index(simap_iteration_dict[x1][y1], simap_iteration_dict[x2][y2])\n",
    "        \n",
    "        j_f12.append(d12)\n",
    "        \n",
    "    jaccard_indices.append(j_f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248122cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "\n",
    "Z = ward(squareform(jaccard_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ffd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = fcluster(Z, t=0.5, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "\n",
    "for i,c in enumerate(clusters):\n",
    "    if c not in cluster_dict: cluster_dict[c] = []\n",
    "    cluster_dict[c].append(fellowship_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f412365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fellowship_list(f_list):\n",
    "    return [simap_iteration_dict[int(index.split('_')[0])][int(index.split('_')[1])] for index in f_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_freq_dict = {}\n",
    "\n",
    "for entry in sorted(cluster_dict.items(), key=lambda kv: len(kv[1]), reverse=True): \n",
    "       \n",
    "    f_list = decode_fellowship_list(entry[1])\n",
    "    f_list = list(itertools.chain.from_iterable(f_list))\n",
    "    \n",
    "    for e,f in Counter(f_list).most_common():\n",
    "        if e not in max_freq_dict: max_freq_dict[e] = f\n",
    "        else: max_freq_dict[e] = max(max_freq_dict[e], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0607f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "visited, counter = [], 0\n",
    "\n",
    "merged_fellowships = []\n",
    "no_merge_fellowships = []\n",
    "\n",
    "for entry in sorted(cluster_dict.items(), key=lambda kv: len(kv[1]), reverse=True): \n",
    "        \n",
    "    f_list = decode_fellowship_list(entry[1])\n",
    "    f_list = list(itertools.chain.from_iterable(f_list))\n",
    "    \n",
    "    print('Entry:', entry[0])\n",
    "    \n",
    "    merge = []\n",
    "    no_merge = []\n",
    "    \n",
    "    for e,f in Counter(f_list).most_common():\n",
    "                \n",
    "        if e not in visited:\n",
    "            if f >= max_freq_dict[e]: \n",
    "                print('- {0:55} {1}'.format(colored(e.replace('http://dbpedia.org/resource/', ''), 'blue'), f))\n",
    "                visited.append(e)\n",
    "                no_merge.append(e)\n",
    "                counter += 1\n",
    "                \n",
    "            else: print('- {0:55} {1}'.format(colored(e.replace('http://dbpedia.org/resource/', ''), 'red'), f))\n",
    "              \n",
    "        else: print('- {0:55} {1}'.format(colored(e.replace('http://dbpedia.org/resource/', ''), 'red'), f))\n",
    "    \n",
    "        if f >= 5: merge.append(e)\n",
    "            \n",
    "    if len(merge) > 0: merged_fellowships.append(merge)\n",
    "    else: no_merge_fellowships.append(no_merge)\n",
    "        \n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e8ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_indices = []\n",
    "\n",
    "for i, f1 in tqdm(list(enumerate(merged_fellowships))):\n",
    "    \n",
    "    j_f12 = []\n",
    "    \n",
    "    for j, f2 in enumerate(merged_fellowships):\n",
    "\n",
    "        d12 = 1.0 - jaccard_index(f1, f2)\n",
    "        \n",
    "        j_f12.append(d12)\n",
    "        \n",
    "    jaccard_indices.append(j_f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "\n",
    "Z = ward(squareform(jaccard_indices))\n",
    "\n",
    "clusters = fcluster(Z, t=0.25, criterion='distance')\n",
    "\n",
    "cluster_dict = {}\n",
    "\n",
    "for i,c in enumerate(clusters):\n",
    "    if c not in cluster_dict: cluster_dict[c] = []\n",
    "    cluster_dict[c].append(merged_fellowships[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_freq_dict = {}\n",
    "\n",
    "for entry in sorted(cluster_dict.items(), key=lambda kv: len(kv[1]), reverse=True): \n",
    "       \n",
    "    f_list = entry[1].copy()\n",
    "    f_list = list(itertools.chain.from_iterable(f_list))\n",
    "    \n",
    "    for e,f in Counter(f_list).most_common():\n",
    "        if e not in max_freq_dict: max_freq_dict[e] = f\n",
    "        else: max_freq_dict[e] = max(max_freq_dict[e], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc380a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_merged_fellowships, visited = [], []\n",
    "\n",
    "for entry in sorted(cluster_dict.items(), key=lambda kv: len(list(itertools.chain.from_iterable(kv[1]))), reverse=True): \n",
    "\n",
    "    f_list = list(itertools.chain.from_iterable(entry[1]))\n",
    "        \n",
    "    print('Entry:', entry[0])\n",
    "    print()\n",
    "    \n",
    "    remerged = []\n",
    "    for e,f in Counter(f_list).most_common():\n",
    "        \n",
    "        if e not in visited:\n",
    "            if f >= max_freq_dict[e]: \n",
    "                print('- {0:55} {1}'.format(colored(e.replace('http://dbpedia.org/resource/', ''), 'blue'), f))\n",
    "                visited.append(e)\n",
    "                remerged.append(e)\n",
    "                \n",
    "            else: print('- {0:55} {1}'.format(colored(e.replace('http://dbpedia.org/resource/', ''), 'red'), f))\n",
    "              \n",
    "        else: print('- {0:55} {1}'.format(colored(e.replace('http://dbpedia.org/resource/', ''), 'red'), f))\n",
    "        \n",
    "    re_merged_fellowships.append(remerged)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e984734",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_dict = {int_to_node[k]:v for k,v in dict(nx.closeness_centrality(G)).items()}\n",
    "\n",
    "produced_fellowships = re_merged_fellowships + no_merge_fellowships\n",
    "produced_fellowships = [f for f in produced_fellowships if len(f) > 0]\n",
    "\n",
    "for f in sorted(produced_fellowships, key=len, reverse=True):\n",
    "    if len(f) == 0: continue\n",
    "    for e in sorted(f, key=lambda e: centrality_dict[e], reverse=True): \n",
    "        print('-', e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab910a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'pkb/' + 'fellowships.pckl', 'wb') as f: pickle.dump(produced_fellowships, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowships = produced_fellowships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Nodes:', G.number_of_nodes())\n",
    "print('Number of Edges:', G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "print('Fellowship Entities:', len(set(itertools.chain.from_iterable(fellowships))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "fellowship_graphs = []\n",
    "\n",
    "for f in fellowships:\n",
    "\n",
    "    f_i = nx.Graph()\n",
    "\n",
    "    for n in f: f_i.add_node(n, label = n)\n",
    "\n",
    "    for e in G.subgraph([node_to_int[n] for n in f]).edges(data=True):        \n",
    "        f_i.add_edge(\n",
    "            int_to_node[e[0]],\n",
    "            int_to_node[e[1]],\n",
    "            weight=e[2]['weight']\n",
    "        )            \n",
    "\n",
    "    fellowship_graphs.append(f_i.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 3))\n",
    "plt.hist([g.number_of_nodes() for g in fellowship_graphs], rwidth=0.95)\n",
    "plt.title('Fellowship Sizes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 3))\n",
    "plt.hist([g.number_of_edges() for g in fellowship_graphs], rwidth=0.95)\n",
    "plt.title('Fellowship Connections')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22134bda",
   "metadata": {},
   "source": [
    "### Identify Fellowship Dipoles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dipole(sag, f_i_j):\n",
    "    f_i, f_j = f_i_j\n",
    "    \n",
    "    int_nodes_1 = [node_to_int[n] for n in fellowship_graphs[f_i].nodes()]\n",
    "    int_nodes_2 = [node_to_int[n] for n in fellowship_graphs[f_j].nodes()]\n",
    "\n",
    "    d_ij = G.subgraph(set(int_nodes_1 + int_nodes_2)).copy()\n",
    "\n",
    "    positive_edges, negative_edges = [], []\n",
    "\n",
    "    for e in d_ij.edges(data=True):\n",
    "        if e[0] in int_nodes_1 and e[1] in int_nodes_1: continue\n",
    "        if e[0] in int_nodes_2 and e[1] in int_nodes_2: continue\n",
    "\n",
    "        if e[2]['weight'] > 0.0: positive_edges.append(e)\n",
    "        elif e[2]['weight'] < 0.0: negative_edges.append(e)\n",
    "\n",
    "    if (len(positive_edges) + len(negative_edges)) == 0: return None\n",
    "    if len(negative_edges) == 0.0: return None\n",
    "\n",
    "    p_positive = len(positive_edges) / (len(positive_edges) + len(negative_edges))\n",
    "    p_negative = len(negative_edges) / (len(positive_edges) + len(negative_edges))\n",
    "\n",
    "    si_sign_G, si_adj_sign_G, si_sign_edgelist, si_int_to_node = G_to_fi(d_ij)\n",
    "    si_f_g, si_f_e, si_t, si_solution_dict = calculate_frustration_index(si_sign_G, si_adj_sign_G, si_sign_edgelist)\n",
    "    \n",
    "    dipole_g = nx.Graph()\n",
    "\n",
    "    for n in d_ij.nodes(): \n",
    "        dipole_g.add_node(int_to_node[n], label=int_to_node[n])\n",
    "\n",
    "    for e in d_ij.edges(data=True):        \n",
    "        dipole_g.add_edge(int_to_node[e[0]], int_to_node[e[1]], weight=e[2]['weight'])             \n",
    "\n",
    "    return [(min(f_i, f_j), max(f_i, f_j)), {\n",
    "        'f_g': si_f_g,\n",
    "        'd_ij': dipole_g.copy(),\n",
    "        'pos': len(positive_edges),\n",
    "        'neg': len(negative_edges),\n",
    "        'simap_1': [int_to_node[n] for n in int_nodes_1],\n",
    "        'simap_2': [int_to_node[n] for n in int_nodes_2],\n",
    "        'int_simap_1': int_nodes_1,\n",
    "        'int_simap_2': int_nodes_2,\n",
    "        'negative_ratio': p_negative,\n",
    "        'positive_ratio': p_positive\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_i_j_list = list(itertools.combinations(list(range(len(fellowship_graphs))), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import subprocess, os, pandas as pd\n",
    "from signed_network_balance import *\n",
    "\n",
    "fellowship_dipoles = [extract_dipole(G, f_i_j) for f_i_j in tqdm(f_i_j_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a82a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'pkb/fellowship_graphs.pckl', 'wb') as f: pickle.dump(fellowship_graphs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'pkb/dipoles.pckl', 'wb') as f: pickle.dump(fellowship_dipoles, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8604afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of dipoles:', len(fellowship_dipoles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91735329",
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowship_dipoles = [d for d in fellowship_dipoles if d]\n",
    "\n",
    "_ = [d for d in fellowship_dipoles if d and d[1]['f_g'] >= 0.7 and d[1]['negative_ratio'] >= 0.5]\n",
    "\n",
    "fellowship_dipoles = _.copy()\n",
    "\n",
    "print('Final number of dipoles:', len(fellowship_dipoles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools, json, jsonpickle\n",
    "\n",
    "spacy_paths = [output_dir + 'spacy/' + p + '/' for p in sorted(os.listdir(output_dir + 'spacy/'))]\n",
    "spacy_paths = list(itertools.chain.from_iterable([[p + _ for _ in os.listdir(p)] for p in spacy_paths]))\n",
    "\n",
    "e_np_paths = [p.replace('spacy', 'entity_np_sentiment_attitudes').replace('.pckl', '.pckl.jsonpckl') for p in spacy_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_np_attitudes(path):\n",
    "\n",
    "    try:\n",
    "        with open(path, 'r') as f: entity_np_sentiment_attitudes = json.load(f)   \n",
    "    except Exception as ex: return {}\n",
    "\n",
    "    dt_str = path.split('/')[-2]\n",
    "    \n",
    "    entity_np_sentiment_attitudes['entity_np_sentiment_attitudes'] = jsonpickle.decode(entity_np_sentiment_attitudes['entity_np_sentiment_attitudes'])\n",
    "    entity_np_sentiment_attitudes = entity_np_sentiment_attitudes['entity_np_sentiment_attitudes']\n",
    "    \n",
    "    for e in entity_np_sentiment_attitudes:\n",
    "        entity_np_sentiment_attitudes[e] = dict(entity_np_sentiment_attitudes[e])\n",
    "        \n",
    "    entity_np_sentiment_attitudes = dict(entity_np_sentiment_attitudes)\n",
    "    \n",
    "    return (dt_str, entity_np_sentiment_attitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, jsonpickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "e_np_attitudes = []\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count() - 8)\n",
    "\n",
    "for attitudes in tqdm(\n",
    "    pool.imap_unordered(load_entity_np_attitudes, e_np_paths),\n",
    "    desc='Load E-NP Attitudes',\n",
    "    total=len(e_np_paths)\n",
    "): e_np_attitudes.append(attitudes)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_np_attitudes = [e_np for e_np in e_np_attitudes if len(e_np) == 2]\n",
    "\n",
    "print('E-NP Attitudes:', len(e_np_attitudes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f652a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_np_attitude_dict = {}\n",
    "\n",
    "for domain, att_dict in tqdm(e_np_attitudes):\n",
    "    \n",
    "    if domain not in e_np_attitude_dict: e_np_attitude_dict[domain] = {}\n",
    "    \n",
    "    for e, np_dict in att_dict.items():\n",
    "        \n",
    "        e = fix_entity_uri(e)\n",
    "        \n",
    "        if e not in e_np_attitude_dict[domain]: e_np_attitude_dict[domain][e] = {}\n",
    "\n",
    "        for np, atts in np_dict.items():\n",
    "\n",
    "            if all(v==0 for v in atts['POSITIVE'] + atts['NEGATIVE']): continue\n",
    "\n",
    "            if np not in e_np_attitude_dict[domain][e]: e_np_attitude_dict[domain][e][np] = {'POSITIVE': [], 'NEGATIVE': []}\n",
    "\n",
    "            e_np_attitude_dict[domain][e][np]['POSITIVE'] += atts['POSITIVE']\n",
    "            e_np_attitude_dict[domain][e][np]['NEGATIVE'] += atts['NEGATIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c764335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "dipole_entitites = list(set(itertools.chain.from_iterable([d[1]['simap_1'] + d[1]['simap_2'] for d in fellowship_dipoles])))\n",
    "\n",
    "print('Total dipole entities:', len(dipole_entitites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_entitites = [fix_entity_uri(e) for e in dipole_entitites]\n",
    "dipole_entitites = [e for e in dipole_entitites if e]\n",
    "\n",
    "print('Number of entities:', len(dipole_entitites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42fd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_domain_nps = {k: [] for k in e_np_attitude_dict}\n",
    "\n",
    "for k in dipole_domain_nps:\n",
    "    \n",
    "    dipole_nps = [list(e_np_attitude_dict[k][e].keys()) for e in dipole_entitites if e in e_np_attitude_dict[k]]\n",
    "    dipole_domain_nps[k] += list(itertools.chain.from_iterable(dipole_nps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_nps = list(set(itertools.chain.from_iterable(list(dipole_domain_nps.values()))))\n",
    "\n",
    "print('Total NPs:', len(dipole_nps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf6323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0448f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, nltk, re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "hyphen_regex = r'(?=\\S+[-])([a-zA-Z-]+)'\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "del english_stopwords[37]\n",
    "\n",
    "def tokenize(text): return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_punctuation(text): \n",
    "    _text = []\n",
    "    for t in tokenize(text):\n",
    "        if not len(re.findall(hyphen_regex, t)) > 0: t = ''.join(c if c not in string.punctuation else ' ' for c in t)\n",
    "        else:\n",
    "            hyphen_parts = t.split('-')\n",
    "            hyphen_parts = [remove_punctuation(_) for _ in hyphen_parts]\n",
    "            t = '-'.join(hyphen_parts)\n",
    "        \n",
    "        t = t.strip()\n",
    "        if len(t) > 0: _text.append(t)\n",
    "\n",
    "    return ' '.join(_text)\n",
    "\n",
    "def remove_trailing(text): return text.strip()\n",
    "\n",
    "def reduce_white_space(text): return re.sub(' +', ' ', text)\n",
    "\n",
    "def to_lower_case(text): return text.lower()\n",
    "\n",
    "def remove_stopwords(tokens): return [t for t in tokens if t not in english_stopwords]\n",
    "\n",
    "def remove_digit_tokens(tokens): return [t for t in tokens if not all(c.isdigit() for c in t)]\n",
    "\n",
    "def lemmatize(np):\n",
    "    blob = TextBlob(np)\n",
    "    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "    word_tag_list = [(w, tag_dict.get(pos[0], 'n')) for w, pos in blob.tags]    \n",
    "    return \" \".join([w.lemmatize(t) for w, t in word_tag_list])\n",
    "\n",
    "def pipeline_func(text, func_list):\n",
    "    for f in func_list: text = f(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_np_dict = {}\n",
    "\n",
    "for np in tqdm(set(noun_phrase_set)):\n",
    "    clean_np_dict[np] = pipeline_func(np, [\n",
    "        lemmatize,\n",
    "        to_lower_case,\n",
    "        remove_punctuation,\n",
    "        remove_trailing,\n",
    "        reduce_white_space,       \n",
    "        tokenize,\n",
    "        remove_digit_tokens,\n",
    "        remove_stopwords,\n",
    "        lambda t: ' '.join(t)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_noun_phrase_list = [np for np in set(clean_np_dict.values()) if len(np) > 1]\n",
    "\n",
    "print('Total Cleaned NPs:', len(clean_noun_phrase_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695157a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_noun_phrase_list = model.encode(\n",
    "    clean_noun_phrase_list,\n",
    "    device='cuda',\n",
    "    show_progress_bar=True,\n",
    "    batch_size = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_np_embeddings_dict = {clean_noun_phrase_list[i]: encoded_noun_phrase_list[i] for i in range(len(clean_noun_phrase_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46145d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def community_detection(\n",
    "    embeddings, threshold=0.75, min_community_size=10, init_max_size=1000\n",
    "):\n",
    "    \n",
    "    index = AnnoyIndex(embeddings.shape[1], \"angular\")\n",
    "    for i, v in tqdm(\n",
    "        enumerate(embeddings),\n",
    "        total=embeddings.shape[0],\n",
    "        desc=\"Building ANN\",\n",
    "    ):\n",
    "        index.add_item(i, v)\n",
    "        \n",
    "    index.build(30)\n",
    "    index.save(\"test.ann\")\n",
    "    \n",
    "    top_k_values = []\n",
    "    for emb_i in tqdm(\n",
    "        range(embeddings.shape[0]), desc=\"Finding Minimum Community Size\"\n",
    "    ):\n",
    "        _, distance = index.get_nns_by_item(\n",
    "            emb_i, min_community_size, include_distances=True\n",
    "        )\n",
    "        top_k_values.append(distance)\n",
    "\n",
    "    top_k_values = 1 - np.array(top_k_values)\n",
    "    \n",
    "    extracted_communities = []\n",
    "    total_entries = embeddings.shape[0]\n",
    "    \n",
    "    for i in tqdm(range(total_entries), desc=\"Extracting Communities\"):\n",
    "        if top_k_values[i][-1] >= threshold:\n",
    "            new_cluster = []\n",
    "\n",
    "            top_idx_large, top_val_large = index.get_nns_by_item(\n",
    "                i, init_max_size, include_distances=True\n",
    "            )\n",
    "            top_val_large = (1 - np.array(top_val_large)).tolist()\n",
    "\n",
    "            if top_val_large[-1] < threshold:\n",
    "                for idx, val in zip(top_idx_large, top_val_large):\n",
    "                    if val < threshold:\n",
    "                        break\n",
    "\n",
    "                    new_cluster.append(idx)\n",
    "            else:\n",
    "                        \n",
    "                min_most_size = min([int(total_entries * 0.5), 10000])\n",
    "                idx_large, val_large = index.get_nns_by_item(\n",
    "                    i, min_most_size, include_distances=True\n",
    "                )\n",
    "                val_large = (1 - np.array(val_large)).tolist()\n",
    "                for idx, val in zip(idx_large, val_large):\n",
    "                    if val >= threshold:\n",
    "                        new_cluster.append(idx)\n",
    "\n",
    "            extracted_communities.append(new_cluster)\n",
    "\n",
    "    extracted_communities = sorted(\n",
    "        extracted_communities, key=lambda x: len(x), reverse=True\n",
    "    )\n",
    "    \n",
    "    unique_communities = []\n",
    "    extracted_ids = set()\n",
    "\n",
    "    for community in extracted_communities:\n",
    "        add_cluster = True\n",
    "        for idx in community:\n",
    "            if idx in extracted_ids:\n",
    "                add_cluster = False\n",
    "                break\n",
    "\n",
    "        if add_cluster:\n",
    "            unique_communities.append(community)\n",
    "            for idx in community:\n",
    "                extracted_ids.add(idx)\n",
    "\n",
    "    return unique_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "clusters = community_detection(\n",
    "    numpy.asarray([clean_np_embeddings_dict[k] for k in clean_noun_phrase_list]),\n",
    "    min_community_size=2,\n",
    "    threshold=0.65,\n",
    "    init_max_size=min(1000, len(clean_noun_phrase_list))\n",
    ")\n",
    "\n",
    "cluster_np_dict = {}\n",
    "\n",
    "for i, cluster in enumerate(clusters): cluster_np_dict[i] = [clean_noun_phrase_list[k] for k in cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Clusters:     ', len(cluster_np_dict))\n",
    "print('Number of Clustered NPs:', len(list(itertools.chain.from_iterable(cluster_np_dict.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_topics_dict = {}\n",
    "\n",
    "for k,v in cluster_np_dict.items():\n",
    "    \n",
    "    for np in v:\n",
    "        \n",
    "        np_topics_dict[np] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb35bb",
   "metadata": {},
   "source": [
    "# Possible Approaches:\n",
    "1. Identify the `attitudes` and `polarization` for `PaCTE` topics with the existance of the exact keywords.\n",
    "2. Expand the existance of keywords using the clustering approach: Cluster the `Dipole NPs` and map with `PaCTE` topics based on the distance from centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb93075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarization_index(atts):     \n",
    "        \n",
    "    A_minus = [t for t in atts if t < 0.0]\n",
    "    A_plus = [t for t in atts if t > 0.0]\n",
    "    \n",
    "    if (len(A_minus) + len(A_plus)) == 0.0: return 0.0\n",
    "    \n",
    "    D_A = abs(\n",
    "        (len(A_plus) / (len(A_plus) + len(A_minus))) - \\\n",
    "        (len(A_minus) / (len(A_plus) + len(A_minus)))\n",
    "    )\n",
    "\n",
    "    gc_minus = numpy.mean(A_minus) if len(A_minus) > 0 else 0.0\n",
    "    gc_plus = numpy.mean(A_plus) if len(A_plus) > 0 else 0.0\n",
    "\n",
    "    gc_d = (abs(gc_plus - gc_minus)) / 2\n",
    "\n",
    "    m = (1-D_A) * gc_d\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403950e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_dipole_attitudes(\n",
    "    dipole_tuple,\n",
    "    entity_np_sentiment_attitudes,\n",
    "    dipole_topic_dict\n",
    "):\n",
    "    fi, fj = dipole_tuple[0]\n",
    "    dipole_dict = dipole_tuple[1]\n",
    "    \n",
    "    if (fi, fj) not in dipole_topic_dict: return []\n",
    "    \n",
    "    fi_entities = dipole_dict['simap_1']\n",
    "    fj_entities = dipole_dict['simap_2'] \n",
    "    \n",
    "    fi_np_attitudes_dict = {}\n",
    "    fj_np_attitudes_dict = {}\n",
    "   \n",
    "    for e in fi_entities:\n",
    "        if e not in entity_np_sentiment_attitudes: continue\n",
    "            \n",
    "        for np, atts in entity_np_sentiment_attitudes[e].items():\n",
    "            if not np in fi_np_attitudes_dict: fi_np_attitudes_dict[np] = []\n",
    "            \n",
    "            sentiment = 0.0\n",
    "            sentiments = []           \n",
    "                        \n",
    "            for i in range(len(atts['POSITIVE'])):\n",
    "                \n",
    "                total_p_n = atts['POSITIVE'][i] + atts['NEGATIVE'][i]\n",
    "                \n",
    "                if total_p_n == 0: sentiments.append(0)\n",
    "                else: sentiments.append(sentiment_threshold_difference(\n",
    "                    atts['POSITIVE'][i] / total_p_n,\n",
    "                    atts['NEGATIVE'][i] / total_p_n\n",
    "                ))\n",
    "                \n",
    "            sentiments = [s for s in sentiments if s != 0]    \n",
    "\n",
    "            if len(sentiments) > 0: sentiment = numpy.median(sentiments)\n",
    "            \n",
    "            fi_np_attitudes_dict[np].append(sentiment)\n",
    "                    \n",
    "    for e in fj_entities:\n",
    "        if e not in entity_np_sentiment_attitudes: continue\n",
    "            \n",
    "        for np, atts in entity_np_sentiment_attitudes[e].items():\n",
    "            if not np in fj_np_attitudes_dict: fj_np_attitudes_dict[np] = []\n",
    "            \n",
    "            sentiment = 0.0\n",
    "            sentiments = []           \n",
    "                        \n",
    "            for i in range(len(atts['POSITIVE'])):\n",
    "                \n",
    "                total_p_n = atts['POSITIVE'][i] + atts['NEGATIVE'][i]\n",
    "                \n",
    "                if total_p_n == 0: sentiments.append(0)\n",
    "                else: sentiments.append(sentiment_threshold_difference(\n",
    "                    atts['POSITIVE'][i] / total_p_n,\n",
    "                    atts['NEGATIVE'][i] / total_p_n\n",
    "                ))\n",
    "                \n",
    "            sentiments = [s for s in sentiments if s != 0]    \n",
    "\n",
    "            if len(sentiments) > 0: sentiment = numpy.median(sentiments)\n",
    "                            \n",
    "            fj_np_attitudes_dict[np].append(sentiment)\n",
    "    \n",
    "    fi_frame_attitudes = {}\n",
    "    fj_frame_attitudes = {}\n",
    "\n",
    "    for np, np_atts in fi_np_attitudes_dict.items():\n",
    "\n",
    "        for ci in dipole_topic_dict[(fi, fj)]['np_clusters']:\n",
    "            if np not in dipole_topic_dict[(fi, fj)]['np_clusters'][ci]: continue\n",
    "            if ci not in fi_frame_attitudes: fi_frame_attitudes[ci] = []\n",
    "            fi_frame_attitudes[ci] += np_atts\n",
    "\n",
    "    for np, np_atts in fj_np_attitudes_dict.items():\n",
    "\n",
    "        for ci in dipole_topic_dict[(fi, fj)]['np_clusters']:\n",
    "            if np not in dipole_topic_dict[(fi, fj)]['np_clusters'][ci]: continue\n",
    "            if ci not in fj_frame_attitudes: fj_frame_attitudes[ci] = []\n",
    "            fj_frame_attitudes[ci] += np_atts\n",
    "\n",
    "    polarization_list = []\n",
    "        \n",
    "    for ci in dipole_topic_dict[(fi, fj)]['np_clusters']:\n",
    "        if ci not in fi_frame_attitudes: continue\n",
    "        if ci not in fj_frame_attitudes: continue\n",
    "                        \n",
    "        polarization_list.append({\n",
    "            'dipole': (fi, fj),\n",
    "            'atts_fi': fi_frame_attitudes[ci],\n",
    "            'atts_fj': fj_frame_attitudes[ci],\n",
    "            'topic': {\n",
    "                'id': ci,\n",
    "                'nps': dipole_topic_dict[(fi, fj)]['np_clusters'][ci]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return polarization_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60df895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dipole_topics(dipole_tuple, entity_np_sentiment_attitudes, np_topics_dict):\n",
    "    \n",
    "    dipole_id, dipole_obj, np_attitudes_dict = dipole_tuple[0], dipole_tuple[1], {}\n",
    "    \n",
    "    for entity in dipole_obj['d_ij'].nodes():\n",
    "        \n",
    "        if entity not in entity_np_sentiment_attitudes: continue\n",
    "            \n",
    "        for np, att_obj in entity_np_sentiment_attitudes[entity].items():\n",
    "                        \n",
    "            if np not in np_attitudes_dict: np_attitudes_dict[np] = {'POSITIVE': [], 'NEGATIVE': []}\n",
    "\n",
    "            np_attitudes_dict[np]['POSITIVE'] += att_obj['POSITIVE'].copy()\n",
    "            np_attitudes_dict[np]['NEGATIVE'] += att_obj['NEGATIVE'].copy()\n",
    "                    \n",
    "    dipole_np_list = list(sorted(np_attitudes_dict.keys()))\n",
    "    dipole_np_labels = {np:set(np_topics_dict[clean_np_dict[np]]) for np in dipole_np_list if np in clean_np_dict and clean_np_dict[np] in np_topics_dict}\n",
    "            \n",
    "    np_attitudes_dict = {k:v for k, v in np_attitudes_dict.items() if k in dipole_np_labels}.copy()\n",
    "        \n",
    "    if len(dipole_np_labels) == 0: return None\n",
    "    \n",
    "    cluster_dict = {}\n",
    "\n",
    "    for k,v in dipole_np_labels.items():\n",
    "        \n",
    "        for _ in v:\n",
    "            if _ not in cluster_dict: cluster_dict[_] = []\n",
    "            cluster_dict[_].append(k)\n",
    "    \n",
    "    return {\n",
    "        'fellowship_1': dipole_id[0],\n",
    "        'fellowship_2': dipole_id[1],\n",
    "        'dipole_topics': {            \n",
    "            'np_attitudes': np_attitudes_dict.copy(),\n",
    "            'np_clusters': dict(cluster_dict).copy()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43123aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_threshold_difference(swn_pos, swn_neg):\n",
    "    swn_pos = abs(swn_pos)\n",
    "    swn_neg = abs(swn_neg)\n",
    "    return numpy.sign(swn_pos - swn_neg) * (abs(swn_pos - swn_neg))\n",
    "\n",
    "def resample_attitudes(atts, n):\n",
    "    total_v, v_ratios = len(atts), {}\n",
    "    \n",
    "    for v in Counter(atts).most_common(): v_ratios[v[0]] = v[1] / total_v\n",
    "    r_atts = list(itertools.chain.from_iterable([[v for i in range(math.floor(n * v_ratios[v]))] for v in v_ratios]))\n",
    "    \n",
    "    return r_atts\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * numpy.array(data)\n",
    "    n = len(a)\n",
    "    m, se = numpy.median(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return (m, [m-h, m+h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_e_np_attitude_dict = {}\n",
    "\n",
    "for d in e_np_attitude_dict:\n",
    "    \n",
    "    for e in e_np_attitude_dict[d]:\n",
    "        if not e: continue\n",
    "        if e not in overall_e_np_attitude_dict: overall_e_np_attitude_dict[e] = {}\n",
    "            \n",
    "        for np, atts in e_np_attitude_dict[d][e].items():\n",
    "            \n",
    "            if np not in overall_e_np_attitude_dict[e]: overall_e_np_attitude_dict[e][np] = {'POSITIVE': [], 'NEGATIVE': []}\n",
    "            overall_e_np_attitude_dict[e][np]['POSITIVE'] += atts['POSITIVE'].copy()\n",
    "            overall_e_np_attitude_dict[e][np]['NEGATIVE'] += atts['NEGATIVE'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916450f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_topics = []\n",
    "\n",
    "for dipole in tqdm(fellowship_dipoles): \n",
    "    if not dipole: continue\n",
    "\n",
    "    d_topics = extract_dipole_topics(dipole, e_np_attitude_dict, np_topics_dict)\n",
    "    if not d_topics: continue\n",
    "\n",
    "    dipole_topics.append(d_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22136a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_topics_dict = {\n",
    "    (d['fellowship_1'], d['fellowship_2']):d['dipole_topics']\n",
    "    for d in dipole_topics if d\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9332ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 0\n",
    "ts_ann = []\n",
    "\n",
    "for d in dipole_topics_dict:\n",
    "    ts += len(dipole_topics_dict[d]['np_clusters'])\n",
    "    ts_ann += dipole_topics_dict[d]['np_clusters'].keys()\n",
    "\n",
    "ts_ann = len(set(ts_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64554946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topics:', ts)\n",
    "print('Annotated Topics:', ts_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c106e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "dipole_topic_attitudes = []\n",
    "\n",
    "for dipole in tqdm(fellowship_dipoles): \n",
    "    if not dipole: continue\n",
    "\n",
    "    dipole_topic_attitudes.append(\n",
    "        undersample_dipole_attitudes(\n",
    "            dipole,\n",
    "            overall_e_np_attitude_dict,\n",
    "            dipole_topics_dict\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a849fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_topic_attitudes = list(itertools.chain.from_iterable(dipole_topic_attitudes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_topic_attitudes = []\n",
    "\n",
    "for dipole_t in dipole_topic_attitudes:\n",
    "    if len(set(dipole_t['atts_fi'])) == 1 and dipole_t['atts_fi'][0] == 0.0: continue\n",
    "    if len(set(dipole_t['atts_fj'])) == 1 and dipole_t['atts_fj'][0] == 0.0: continue\n",
    "\n",
    "    filtered_topic_attitudes.append(dipole_t.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i, dipole_t in tqdm(list(enumerate(filtered_topic_attitudes))):\n",
    "\n",
    "    ################################################################\n",
    "    # Remove any 0.0 attitudes from Fi and Fj for the resampling.  #                               \n",
    "    # This code might also remove from original dipole_t object.   #\n",
    "    ################################################################\n",
    "\n",
    "    dipole_t['atts_fi'] = [v for v in dipole_t['atts_fi'] if v != 0.0]\n",
    "    dipole_t['atts_fj'] = [v for v in dipole_t['atts_fj'] if v != 0.0]\n",
    "\n",
    "    if len(dipole_t['atts_fi']) == 0 or len(dipole_t['atts_fj']) == 0: continue\n",
    "\n",
    "    ###########################################################\n",
    "    # If Fi and Fj attitudes have the same size then they do  #\n",
    "    # not need resampling.                                    #\n",
    "    ###########################################################\n",
    "\n",
    "    if len(dipole_t['atts_fi']) == len(dipole_t['atts_fj']):\n",
    "        filtered_topic_attitudes[i]['X'] = dipole_t['atts_fi'] + dipole_t['atts_fj']\n",
    "        filtered_topic_attitudes[i]['pi'] = calculate_polarization_index(filtered_topic_attitudes[i]['X'])\n",
    "    else:\n",
    "\n",
    "        if len(dipole_t['atts_fi']) > len(dipole_t['atts_fj']):\n",
    "\n",
    "            fj_res = resample_attitudes(dipole_t['atts_fj'], len(dipole_t['atts_fi']))\n",
    "\n",
    "            filtered_topic_attitudes[i]['X']     = dipole_t['atts_fi'] + dipole_t['atts_fj']\n",
    "            filtered_topic_attitudes[i]['X_res'] = dipole_t['atts_fi'] + fj_res\n",
    "\n",
    "            filtered_topic_attitudes[i]['pi'] = calculate_polarization_index(\n",
    "                filtered_topic_attitudes[i]['X']\n",
    "            )\n",
    "\n",
    "            filtered_topic_attitudes[i]['pi_res'] = calculate_polarization_index(\n",
    "                filtered_topic_attitudes[i]['X_res']\n",
    "            )\n",
    "\n",
    "        else: \n",
    "\n",
    "            fi_res = resample_attitudes(dipole_t['atts_fi'], len(dipole_t['atts_fj']))\n",
    "\n",
    "            filtered_topic_attitudes[i]['X']     = dipole_t['atts_fi'] + dipole_t['atts_fj']\n",
    "            filtered_topic_attitudes[i]['X_res'] = dipole_t['atts_fj'] + fi_res\n",
    "\n",
    "            filtered_topic_attitudes[i]['pi'] = calculate_polarization_index(\n",
    "                filtered_topic_attitudes[i]['X']\n",
    "            )\n",
    "\n",
    "            filtered_topic_attitudes[i]['pi_res'] = calculate_polarization_index(\n",
    "                filtered_topic_attitudes[i]['X_res']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_polarization_dict = {}\n",
    "\n",
    "for dipole_t in filtered_topic_attitudes:\n",
    "    if dipole_t['topic']['id'] not in topic_to_polarization_dict: topic_to_polarization_dict[dipole_t['topic']['id']] = []\n",
    "    topic_to_polarization_dict[dipole_t['topic']['id']].append(dipole_t['pi_res'] if 'pi_res' in dipole_t else dipole_t['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:^5} {1:^80} {2:^5} {3:^5} {4:^5} {5:^5} {6:^5}'.format(\n",
    "    'No.', \n",
    "    'Topic',\n",
    "    '#D',\n",
    "    'Median',\n",
    "    'From',\n",
    "    'To',\n",
    "    'Score'\n",
    "))\n",
    "\n",
    "print('='.join(['' for i in range(120)]))\n",
    "\n",
    "for i, t in enumerate(sorted(\n",
    "    topic_to_polarization_dict.items(),\n",
    "    key=lambda kv: len(kv[1]) * numpy.median(kv[1]),\n",
    "    reverse=True\n",
    ")):\n",
    "\n",
    "    pis = t[1]\n",
    "    t = t[0]\n",
    "    t_i = t\n",
    "    t = pacte_top_10[t]\n",
    "    \n",
    "    m, h_m_p = mean_confidence_interval(pis)\n",
    "    h_m = h_m_p[0]\n",
    "    h_p = h_m_p[1]\n",
    "    \n",
    "    print(colored('{0:5}. {1:80} {2:<5} {3:<5} {4:<5} {5:<5} {6:<5}'.format(\n",
    "        i + 1, \n",
    "        str(str(t) + ' = ' + ', '.join(cluster_np_dict[t_i]))[:80],\n",
    "        len(pis),\n",
    "        round(m, 2),\n",
    "        round(h_m, 2),\n",
    "        round(h_p, 2),\n",
    "        round(\n",
    "            len(pis) * numpy.median(pis), 2\n",
    "        )\n",
    "    ), 'blue'))\n",
    "            \n",
    "    print('-'.join(['' for i in range(120)]))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
