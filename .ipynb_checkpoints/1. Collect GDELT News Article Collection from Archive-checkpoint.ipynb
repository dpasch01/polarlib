{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdfad60",
   "metadata": {},
   "source": [
    "# Periodically Collect and Filter GDELT Archives\n",
    "This notebook is the main code for parsing and retrieving news article archives from the [GDELT](http://data.gdeltproject.org/events/) project, as well as filtering these archives based on the type of actors and other GDELT specific properties for the analysis at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './polar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859c384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c49a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "starting_date = date(year = 2020, month = 2, day = 1)\n",
    "ending_date = date(year = 2020, month = 8, day = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_duration = ending_date - starting_date\n",
    "days_duration = days_duration.days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting date: ' + str(starting_date))\n",
    "print('Ending date: ' + str(ending_date))\n",
    "print()\n",
    "\n",
    "print('Total number of days: ' + str(days_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfa026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66beb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "base = 'http://data.gdeltproject.org/events/{}.export.CSV.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fecc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isdir(output_dir): print('Warning: Path \\'%s\\' already exists.' % output_dir)\n",
    "else: \n",
    "    os.makedirs(output_dir)\n",
    "    os.makedirs(output_dir + 'dumps')\n",
    "    os.makedirs(output_dir + 'html')\n",
    "    os.makedirs(output_dir + 'articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(days_duration), desc='Retrieving GDELT Dumps'):\n",
    "    d = starting_date + timedelta(days=i)\n",
    "    d_str = d.strftime('%Y%m%d')\n",
    "\n",
    "    wget.download(base.format(d_str), out=output_dir + 'dumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print('Succesfully collected %d GDELT archives.' % len([g for g in os.listdir(output_dir + 'dumps') if g.endswith('CSV.zip')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5d9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_fields = [\n",
    "    'globaleventid', 'day', 'monthyear', 'year', 'fractiondate', 'actor1code', 'actor1name', 'actor1countrycode',\n",
    "    'actor1knowngroupcode', 'actor1ethniccode', 'actor1religion1code', 'actor1religion2code', 'actor1type1code',\n",
    "    'actor1type2code', 'actor1type3code', 'actor2code', 'actor2name', 'actor2countrycode', 'actor2knowngroupcode',\n",
    "    'actor2ethniccode', 'actor2religion1code', 'actor2religion2code', 'actor2type1code', 'actor2type2code', \n",
    "    'actor2type3code', 'isrootevent', 'eventcode', 'eventbasecode', 'eventrootcode', 'quadclass', 'goldsteinscale', \n",
    "    'nummentions', 'numsources', 'numarticles', 'avgtone', 'actor1geo_type', 'actor1geo_fullname', \n",
    "    'actor1geo_countrycode', 'actor1geo_adm1code', 'actor1geo_lat', 'actor1geo_long', 'actor1geo_featureid', \n",
    "    'actor2geo_type', 'actor2geo_fullname', 'actor2geo_countrycode', 'actor2geo_adm1code string', 'actor2geo_lat', \n",
    "    'actor2geo_long', 'actor2geo_featureid', 'actiongeo_type', 'actiongeo_fullname', 'actiongeo_countrycode', \n",
    "    'actiongeo_adm1code', 'actiongeo_lat', 'actiongeo_long', 'actiongeo_featureid', 'dateadded', 'sourceurl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, numpy\n",
    "\n",
    "def get_source_path(sourceurl): \n",
    "    if not isinstance(sourceurl, str) and numpy.isnan(sourceurl): return ''\n",
    "    return urllib.parse.urlparse(sourceurl).path    \n",
    "\n",
    "def get_source(sourceurl): \n",
    "    if not isinstance(sourceurl, str) and numpy.isnan(sourceurl): return ''\n",
    "    return urllib.parse.urlparse(sourceurl).netloc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_url(url, gd_day, archive_flag = True):\n",
    "    if archive_flag: return 'https://web.archive.org/web/' + str(gd_day) + '00000/' + url  \n",
    "    else: return url  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf31c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "config = Config()\n",
    "\n",
    "config.browser_user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "config.request_timeout = 3\n",
    "\n",
    "def collect_article(article_url, parse_flag=True, nlp_flag=False):\n",
    "    article = Article(article_url, config=config)\n",
    "    article.download()\n",
    "    if parse_flag: article.parse()\n",
    "    if parse_flag and nlp_flag: article.nlp()\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "\n",
    "def prepare_title(s):\n",
    "    for st in string.punctuation: s = s.replace(st, ' ')\n",
    "    s = re.sub(' +', '-', s)\n",
    "    s = s.lower()\n",
    "    \n",
    "    return s[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, pandas as pd, requests\n",
    "\n",
    "from threading import Thread\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b083739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from requests.exceptions import ConnectionError, InvalidSchema, MissingSchema, TooManyRedirects, RetryError\n",
    "\n",
    "def fetch_article_task(q):\n",
    "\n",
    "    while not q.empty():\n",
    "\n",
    "        idx, hgd = q.get()\n",
    "        \n",
    "        archive_url = generate_query_url(\n",
    "            hgd['sourceurl'],\n",
    "            hgd['day'],\n",
    "            archive_flag=False\n",
    "        )   \n",
    "\n",
    "        hgd['config_day'] = d_str\n",
    "        \n",
    "        try: article_obj = collect_article(archive_url, parse_flag=False)\n",
    "        except Exception as ex: \n",
    "            print(idx, archive_url, ex)\n",
    "            pass\n",
    "                        \n",
    "        output_folder = output_dir + 'html/' + str(hgd['config_day']) + '/'\n",
    "        output_file = output_folder + '.' + hgd['source'] + '.' + prepare_title(get_source_path(archive_url))[:100] + '.html'\n",
    "        if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "        with open(output_file, 'w') as html_file: html_file.write(article_obj.html)\n",
    "                            \n",
    "        time.sleep(0.100)\n",
    "        \n",
    "        q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time, sys\n",
    "\n",
    "for i in range(days_duration):\n",
    "    d = starting_date + timedelta(days=i)\n",
    "        \n",
    "    d_str = d.strftime('%Y%m%d')  \n",
    "    \n",
    "    zf = zipfile.ZipFile('{}{}.export.CSV.zip'.format(output_dir + 'dumps/', d_str)) \n",
    "\n",
    "    gd_df = pd.read_csv(zf.open('{}.export.CSV'.format(d_str)), sep='\\t', header=None)\n",
    "    gd_df.columns = gdelt_fields\n",
    "        \n",
    "    gd_df['sourceurl_path'] = gd_df['sourceurl'].apply(get_source_path)   \n",
    "    gd_df['source'] = gd_df['sourceurl'].apply(get_source)   \n",
    "      \n",
    "    #######################################\n",
    "    # Here add your filters for the GDELT #\n",
    "    # articles. For example, I want the   # \n",
    "    # articles to be related to the US.   #\n",
    "    #######################################\n",
    "        \n",
    "    scope_df = gd_df[(\n",
    "        (gd_df['actor1countrycode']=='USA') | \n",
    "        (gd_df['actor2countrycode']=='USA')\n",
    "    )]\n",
    "    \n",
    "    scope_df = scope_df.sort_values(by = ['numarticles'], ascending=False)\n",
    "    \n",
    "    scope_df = scope_df.head(5000)\n",
    "    \n",
    "    article_n = len(set(scope_df['sourceurl'].values))\n",
    "    scope_df = list(scope_df.T.to_dict().values())\n",
    "    \n",
    "    q = Queue(maxsize=0)\n",
    "    threads = min(128, article_n)\n",
    "\n",
    "    sys.stdout.write('- Fetching {} articles for: {}'.format(article_n, d.strftime('%Y %m %d')))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if article_n == 0: os.makedirs(output_dir + 'html/' + d_str, exist_ok=True)\n",
    "    \n",
    "    for j in range(article_n): q.put((j, scope_df[j]))\n",
    "        \n",
    "    t0 = time.time()\n",
    "\n",
    "    for k in range(threads):\n",
    "        thread = Thread(target=fetch_article_task, args=[q])\n",
    "        thread.setDaemon(True)\n",
    "        thread.start()\n",
    "\n",
    "    q.join()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    sys.stdout.write(' [{}s]'.format(round(t1 - t0, 6)))\n",
    "    sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Process, Manager\n",
    "import json\n",
    "\n",
    "def parse_html(file_path):\n",
    "        \n",
    "    with open(file_path, 'r') as f: html_content = f.read()\n",
    "        \n",
    "    hgd = file_path.split('/')[-2]\n",
    "    uid = file_path.split('/')[-1].replace('.html', '')\n",
    "        \n",
    "    if len(html_content) == 0: return None\n",
    "\n",
    "    article = Article('', language='en')\n",
    "    article.download(input_html=html_content)\n",
    "    article.parse()\n",
    "    \n",
    "    hgd_dt = datetime.strptime(hgd, '%Y%m%d')\n",
    "    \n",
    "    article_dict = {\n",
    "        'url': article.url,\n",
    "        'uid': uid,\n",
    "        'images': list(article.images),\n",
    "        'publication-date': article.publish_date.strftime('%Y-%m-%d') if article.publish_date else hgd_dt.strftime('%Y-%m-%d'),\n",
    "        'text': article.text,\n",
    "        'title': article.title,\n",
    "        'top-image': article.top_image\n",
    "    }\n",
    "        \n",
    "    article_dict_str = json.dumps(article_dict, indent=4)\n",
    "    \n",
    "    output_folder = output_dir + 'articles/' + hgd + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as html_file: html_file.write(article_dict_str)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530809b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for i in tqdm(list(range(days_duration))):\n",
    "    \n",
    "    d = starting_date + timedelta(days=i)\n",
    "    d_str = d.strftime('%Y%m%d')  \n",
    "    \n",
    "    daily_path = output_dir + 'html/' + d_str + '/'\n",
    "    \n",
    "    file_paths += [daily_path + p for p in os.listdir(daily_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9127bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(parse_html, file_paths),\n",
    "    desc='HTML Parsing',\n",
    "    total=len(file_paths)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c274c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path_list = []\n",
    "\n",
    "for root, folders, files in os.walk(f'{output_dir}articles'):\n",
    "    if len(files) == 0: continue\n",
    "    for f in files: article_path_list.append(root + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Articles:', len(article_path_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
