{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255aeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './polar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db087b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools, json\n",
    "\n",
    "article_paths = [output_dir + 'articles/' + p + '/' for p in sorted(os.listdir(output_dir + 'articles/'))]\n",
    "article_paths = list(itertools.chain.from_iterable([[p + _ for _ in os.listdir(p)] for p in article_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78886c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of articles:', len(article_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74678634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee92e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special(text):\n",
    "    text = text.replace('``', \"''\")\n",
    "    text = text.replace('`', \"'\")\n",
    "    text = text.replace('“', '\"')\n",
    "    text = text.replace('”', '\"')\n",
    "    text = text.replace('’', \"'\")\n",
    "    text = text.replace('‘', \"'\")\n",
    "    text = text.replace(\"'\", \"'\")\n",
    "    text = text.replace('–', \"-\")\n",
    "    text = text.replace('—', \"-\")\n",
    "    text = text.replace('\\\"', '\"')\n",
    "    text = text.replace(\"\\'\", \"'\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def encode(text): return text.encode(encoding=\"ascii\",errors=\"ignore\")\n",
    "def decode(text): return text.decode(\"utf-8\")\n",
    "\n",
    "def uncontract(text):    \n",
    "    text = re.sub(r\"(\\b)([Aa]re|[Cc]ould|[Dd]id|[Dd]oes|[Dd]o|[Hh]ad|[Hh]as|[Hh]ave|[Ii]s|[Mm]ight|[Mm]ust|[Ss]hould|[Ww]ere|[Ww]ould)n't\", r\"\\1\\2 not\", text)\n",
    "    text = re.sub(r\"(\\b)([Hh]e|[Ii]|[Ss]he|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou)'ll\", r\"\\1\\2 will\", text)\n",
    "    text = re.sub(r\"(\\b)([Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou)'re\", r\"\\1\\2 are\", text)\n",
    "    text = re.sub(r\"(\\b)([Ii]|[Ss]hould|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Ww]ould|[Yy]ou)'ve\", r\"\\1\\2 have\", text)\n",
    "    \n",
    "    text = re.sub(r\"(\\b)([Cc]a)n't\", r\"\\1\\2n not\", text)\n",
    "    text = re.sub(r\"(\\b)([Ii])'m\", r\"\\1\\2 am\", text)\n",
    "    text = re.sub(r\"(\\b)([Ll]et)'s\", r\"\\1\\2 us\", text)\n",
    "    text = re.sub(r\"(\\b)([Tt]here)'s\", r\"\\1\\2 is\", text)\n",
    "    text = re.sub(r\"(\\b)([Ww])on't\", r\"\\1\\2ill not\", text)\n",
    "    text = re.sub(r\"(\\b)([Ss])han't\", r\"\\1\\2hall not\", text)\n",
    "    text = re.sub(r\"(\\b)([Yy])(?:'all|a'll)\", r\"\\1\\2ou all\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pipeline_func(text, func_list):\n",
    "    for f in func_list: text = f(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544345b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "\n",
    "def load_gzip(path, func=json.loads):\n",
    "    with gzip.open(path, 'r') as f: data = func(f.read().decode('utf-8'))\n",
    "    return data\n",
    "\n",
    "def pre_process_article(path):\n",
    "\n",
    "    with open(path, 'r') as f: article_obj = json.load(f)\n",
    "\n",
    "    article_dict_str = json.dumps({\n",
    "        'uid': article_obj['uid'],\n",
    "        'text': pipeline_func(\n",
    "            article_obj['text'],\n",
    "            [replace_special, uncontract, lambda t: t.replace('\\n', ' ')]\n",
    "        )\n",
    "    })\n",
    "\n",
    "    output_folder = output_dir + 'pre_processed/' + path.split('/')[4] + '/'\n",
    "    output_file = output_folder + article_obj['uid'] + '.json.gzip'    \n",
    "    \n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with gzip.open(output_file, 'w') as f: f.write(article_dict_str.encode('utf-8'))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing, re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count())\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(\n",
    "        pre_process_article,\n",
    "        article_paths\n",
    "    ),\n",
    "    desc='Article Pre-processing',\n",
    "    total=len(article_paths)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac8a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825db0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "import pickle, spacy\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "def save_spacy_obj(spacy_doc, store_path):\n",
    "    spacy_doc_bytes = spacy_doc.to_bytes()\n",
    "    with open(store_path, 'wb') as f: pickle.dump(spacy_doc_bytes, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def save_spacy_vocab(store_path):\n",
    "    vocab_bytes = spacy_nlp.vocab.to_bytes()\n",
    "    with open(store_path, 'wb') as f: pickle.dump(vocab_bytes, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def load_spacy_obj(load_path, vocab):\n",
    "    with open(load_path, 'rb') as f: spacy_doc_bytes = pickle.load(f)\n",
    "    spacy_doc = Doc(vocab).from_bytes(spacy_doc_bytes)\n",
    "    return spacy_doc\n",
    "    \n",
    "def load_spacy_vocab(load_path):\n",
    "    vocab = Vocab()\n",
    "    \n",
    "    with open(load_path, 'rb') as f: vocab_bytes = pickle.load(f)\n",
    "    vocab.from_bytes(vocab_bytes)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_trf\")    \n",
    "\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "for i, c in enumerate(spacy_nlp.pipe_names): print('-', str(i + 1) + '.', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa719ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "pre_processed_paths = [[o1 + '/' + p for p in o3] for o1, o2, o3 in os.walk(output_dir + 'pre_processed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_paths = list(itertools.chain.from_iterable(pre_processed_paths))\n",
    "\n",
    "print('Number of pre-processed:', len(pre_processed_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06803b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_to_disk(path):\n",
    "        \n",
    "    article_obj = load_gzip(path, func=json.loads)\n",
    "    if len(article_obj['text'].strip()) == 0: return None\n",
    "    \n",
    "    doc = spacy_nlp(article_obj['text'])\n",
    "    doc._.trf_data = None\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    output_folder = output_dir + 'spacy/' + path.split('/')[4] + '/'\n",
    "        \n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)    \n",
    "    save_spacy_obj(doc, output_folder + '{}.pckl'.format(article_obj['uid']))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "for pp_path in tqdm(pre_processed_paths): \n",
    "    try: spacy_to_disk(pp_path)    \n",
    "    except Exception as ex:\n",
    "        print('Path:', pp_path)\n",
    "        print('Exception:', ex)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072d759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daef917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools, json\n",
    "\n",
    "spacy_paths = [output_dir + 'spacy/' + p + '/' for p in sorted(os.listdir(output_dir + 'spacy/'))]\n",
    "spacy_paths = list(itertools.chain.from_iterable([[p + _ for _ in os.listdir(p)] for p in spacy_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ade6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of SpaCy articles:', len(spacy_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af694dbb",
   "metadata": {},
   "source": [
    "Before executing `get_dbpedia_entities` start the `Spotlight` containerized service.\n",
    "\n",
    "`$ docker run -tid --restart unless-stopped --name dbpedia-spotlight.en --mount source=spotlight-model,target=/opt/spotlight -p 2222:80 dbpedia/dbpedia-spotlight spotlight.sh en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, spotlight\n",
    "from spotlight import SpotlightException\n",
    "\n",
    "def get_dbpedia_entities(text, confidence = 0.45):\n",
    "    \n",
    "    req_data = {'lang': 'en', 'text': str(text), 'confidence': confidence, 'types': ['']}\n",
    "    spot_entities = requests.post('http://127.0.0.1:2222/rest/annotate', data=req_data, headers = {\"Accept\": \"application/json\"})\n",
    "    \n",
    "    try: \n",
    "        if 'Resources' not in spot_entities.json(): raise SpotlightException()\n",
    "        spot_entities = [{k[1:]:v for k,v in r.items()} for r in spot_entities.json()['Resources']]\n",
    "    except SpotlightException as se: return []\n",
    "        \n",
    "    return [{  \n",
    "        'begin': int(e['offset']),\n",
    "        'end': int(e['offset']) + len(e['surfaceForm']),\n",
    "        'title': e['URI'],\n",
    "        'score': float(e['similarityScore']),\n",
    "        'text': e['surfaceForm'],\n",
    "        'types': [t.replace('Wikidata:', '') for t in e['types'].split(',') if 'Wikidata' in t],\n",
    "        'wikid': e['URI'],\n",
    "        'dbpedia': e['URI']      \n",
    "    } for e in spot_entities]\n",
    "\n",
    "    time.sleep(0.250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8869348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66904eab",
   "metadata": {},
   "source": [
    "For additional accuracy, use the d4science `WAT` service. Sign-up to get your own `GCUBE_TOKEN` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1430ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "GCUBE_TOKEN = 'GCUBE_TOKEN_API'\n",
    "\n",
    "def get_wat_entities(text, confidence = 0.2):\n",
    "    \n",
    "    wat_return = requests.get('https://wat.d4science.org/wat/tag/tag', params={\n",
    "        'lang': 'en',\n",
    "        'gcube-token': GCUBE_TOKEN,\n",
    "        'text': text\n",
    "    })\n",
    "    \n",
    "    wat_return = wat_return.json()\n",
    "    \n",
    "    if not 'annotations' in wat_return: raise Exception('No annotations have been found.')\n",
    "    return [{\n",
    "        'begin': wats['start'],\n",
    "        'end': wats['end'],\n",
    "        'title': wats['title'],\n",
    "        'score': wats['rho'],\n",
    "        'text': wats['spot'],\n",
    "        'wikid': wats['id'],\n",
    "        'dbpedia': 'http://dbpedia.org/resource/' + wats['title']        \n",
    "    } for wats in wat_return['annotations'] if wats['rho'] >= confidence]\n",
    "\n",
    "    time.sleep(0.750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cec354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_resources(e_list):\n",
    "    wat_mentions = defaultdict(lambda: [])\n",
    "    dbpedia_mentions = defaultdict(lambda: [])\n",
    "\n",
    "    t = ' and '.join(e_list)\n",
    "    if len(t) == 0: return None\n",
    "    \n",
    "    try: \n",
    "        \n",
    "        wat_entities = get_wat_entities(t, confidence=0.5)\n",
    "        for e in wat_entities: wat_mentions[e['text']].append(e['dbpedia'])\n",
    "            \n",
    "    except ProxyError: \n",
    "        \n",
    "        time.sleep(0.750)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            wat_entities = get_wat_entities(t, confidence=0.5)\n",
    "            for e in wat_entities: wat_mentions[e['text']].append(e['dbpedia'])\n",
    "        \n",
    "        except ProxyError:\n",
    "            \n",
    "            wat_mentions = {\n",
    "                'message': 'ProxyError',\n",
    "                'input': t\n",
    "            }      \n",
    "        \n",
    "    dbpedia_entities = get_dbpedia_entities(t, confidence=0.45)\n",
    "    for e in dbpedia_entities: dbpedia_mentions[e['text']].append(e['dbpedia'])\n",
    "        \n",
    "    return (dict(wat_mentions), dict(dbpedia_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_types = ['PERSON', 'LOC', 'NORP', 'ORG', 'GPE', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52389fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_entity_pairs(path):\n",
    "    \n",
    "    doc = load_spacy_obj(path, spacy_nlp.vocab)\n",
    "    \n",
    "    ner_pairs = []\n",
    "\n",
    "    for np in doc.noun_chunks:\n",
    "        \n",
    "        for e in np.ents:\n",
    "            \n",
    "            if e.label_ in ner_types: ner_pairs.append(e.text)\n",
    "\n",
    "    return ner_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ab17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, jsonpickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count()-32)\n",
    "\n",
    "ner_entries = []\n",
    "\n",
    "for e_np_ents in tqdm(\n",
    "    pool.imap_unordered(export_entity_pairs, spacy_paths),\n",
    "    desc='Loading Entities',\n",
    "    total=len(spacy_paths)\n",
    "): \n",
    "    ner_entries.append(e_np_ents)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "from requests.exceptions import ProxyError\n",
    "\n",
    "pool = Pool(16)\n",
    "\n",
    "wat_mentions = []\n",
    "dbpedia_mentions = []\n",
    "\n",
    "for wd in tqdm(\n",
    "    pool.imap_unordered(get_mention_resources, ner_entries),\n",
    "    desc='Fetching Entity Mentions',\n",
    "    total=len(ner_entries)\n",
    "): \n",
    "    if not wd: continue\n",
    "    wat_mentions.append(wd[0])\n",
    "    dbpedia_mentions.append(wd[1])\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3779c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "wat_fails = [w['input'] for w in wat_mentions if 'message' in w]\n",
    "\n",
    "print('Proxy Errors:', len(wat_fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e72c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "wat_mentions_freq = defaultdict(lambda: [])\n",
    "dbpedia_mentions_freq = defaultdict(lambda: [])\n",
    "        \n",
    "for wm in wat_mentions:\n",
    "    for w, m in wm.items(): wat_mentions_freq[w] += m\n",
    "\n",
    "for dm in dbpedia_mentions:\n",
    "    for d, m in dm.items(): dbpedia_mentions_freq[d] += m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a158172",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = list(itertools.chain.from_iterable(ner_entries))\n",
    "\n",
    "entity_set = list(set(entity_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b045896",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_linking_dict = {}\n",
    "\n",
    "for e in tqdm(entity_set):\n",
    "    entity_linking_dict[e] = []\n",
    "    entity_linking_dict[e] += [r.replace('dbpedia', 'wat') for r in wat_mentions_freq[e]]\n",
    "    entity_linking_dict[e] += dbpedia_mentions_freq[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14055016",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'entity_linking_dict.pckl', 'wb') as f: pickle.dump(entity_linking_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3088fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def discover_entities(path):\n",
    "    \n",
    "    domain_folder = path.split('/')[-2]\n",
    "    uid = path.split('/')[-1].replace('.pckl', '')\n",
    "    \n",
    "    output_folder = output_dir + 'entities/' + domain_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    \n",
    "    if os.path.exists(output_file): return None\n",
    "        \n",
    "    spacy_doc = load_spacy_obj(path, spacy_nlp.vocab)\n",
    "    \n",
    "    entity_list = []\n",
    "        \n",
    "    for e in spacy_doc.ents:\n",
    "\n",
    "        if not e.label_ in ner_types: continue\n",
    "        if not e.text in entity_linking_dict or len(entity_linking_dict[e.text]) == 0: continue\n",
    "            \n",
    "        uri_freq = Counter(entity_linking_dict[e.text]).most_common(1)[0]\n",
    "            \n",
    "        entity_list.append({\n",
    "            'begin': int(e.start_char),\n",
    "            'end': int(e.end_char),\n",
    "            'title': uri_freq[0],\n",
    "            'score': uri_freq[1],\n",
    "            'text': e.text,\n",
    "            'types': [e.label_],\n",
    "            'wikid': uri_freq[0],\n",
    "            'dbpedia': uri_freq[0]   \n",
    "        })\n",
    "\n",
    "    output_folder = output_dir + 'entities/' + domain_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    \n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'entities': entity_list\n",
    "    }))\n",
    "        \n",
    "    return True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing, re, os, json\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count()-2)\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(discover_entities, spacy_paths),\n",
    "    desc='Bringing Articles to the Spotlight',\n",
    "    total=len(spacy_paths)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_paths = [output_dir + 'entities/' + p + '/' for p in sorted(os.listdir(output_dir + 'entities/'))]\n",
    "entity_paths = list(itertools.chain.from_iterable([[p + _ for _ in os.listdir(p)] for p in entity_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6f89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_token_indices(spacy_document):\n",
    "    sentence_aware_token_indices = {}\n",
    "    for i, sentence in enumerate(spacy_document.sents):\n",
    "        for j, token in enumerate(sentence): \n",
    "            sentence_aware_token_indices[token.i] = j\n",
    "            \n",
    "    return sentence_aware_token_indices\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def assign_entity_2_span(span, entities):\n",
    "    x = range(span[0], span[1])\n",
    "    for e in entities:\n",
    "        y, xs = range(e['begin'], e['end']), set(x)\n",
    "        if len(xs.intersection(y)) > 0: return e\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9ac62",
   "metadata": {},
   "source": [
    "For `coreference resolution` start the `Neuralcoref Service` from here: https://github.com/dpasch01/neuralcoref-service\n",
    "\n",
    "To start execute `$ docker-compose up`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coreferences(text, host='http://127.0.0.1:8150/coref'): return requests.post(host, data={'text': text}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd751e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e369e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coref_clusters(spacy_document, coref_annotations, entity_list):\n",
    "    coreference_clusters = []\n",
    "    \n",
    "    for cc in coref_annotations['coref']:\n",
    "        \n",
    "        cchain_obj = {'chain': [], 'main': {\n",
    "                'start': cc['main']['start'], 'end': cc['main']['end'], 'text': cc['main']['text'],\n",
    "                'entity': assign_entity_2_span((cc['main']['start'], cc['main']['end']), entity_list),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for m in cc['chain']: \n",
    "            _cchain_mention = { 'start': m['start'], 'end': m['end'],\n",
    "                'entity': assign_entity_2_span((m['start'], m['end']), entity_list),\n",
    "                'text': m['text']\n",
    "            }\n",
    "\n",
    "            cchain_obj['chain'].append(_cchain_mention)\n",
    "\n",
    "        coreference_clusters.append(cchain_obj)\n",
    "    \n",
    "    return coreference_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_coref_main(coref_cluster):\n",
    "    max_freq = 0\n",
    "    coref_dict = defaultdict(lambda: {'scores': [], 'frequency': 0})\n",
    "    \n",
    "    for c in coref_cluster['chain']:\n",
    "        if not c['entity']: continue\n",
    "        e = c['entity']\n",
    "        \n",
    "        coref_dict[e['title']]['scores'].append(e['score'])\n",
    "        coref_dict[e['title']]['frequency'] += 1\n",
    "        \n",
    "        max_freq = max(coref_dict[e['title']]['frequency'], max_freq)\n",
    "    \n",
    "    freq_counter = dict(Counter([coref_dict[c]['frequency'] for c in coref_dict]))\n",
    "    \n",
    "    if max_freq == 0: return None\n",
    "    elif freq_counter[max_freq] == 1: \n",
    "        for title in coref_dict:\n",
    "            if coref_dict[title]['frequency'] == max_freq: \n",
    "                return title\n",
    "                break\n",
    "    else:\n",
    "        max_score = 0\n",
    "        max_title = None\n",
    "        \n",
    "        for title in coref_dict:\n",
    "            score = np.mean(coref_dict[title]['scores'])\n",
    "            if score > max_score: \n",
    "                max_score = score\n",
    "                max_title = title\n",
    "                \n",
    "        return max_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def transform_to_core_nlp_obj(spacy_document, document_coreferences, sentence_aware_token_indices, article_entities, np_flag=True):\n",
    "    \n",
    "    spacy_annotation_obj, spacy_dependencies_obj = {}, {}\n",
    "\n",
    "    for i, sentence in enumerate(spacy_document.sents):\n",
    "        spacy_annotation_obj[i], spacy_dependencies_obj[i] = {'index': i, 'tokens': []}, []\n",
    "        \n",
    "        if np_flag: \n",
    "            \n",
    "            np_indices_dict = {np_tokens.text: [np.i for np in np_tokens] for np_tokens in sentence.noun_chunks}\n",
    "\n",
    "            for e_obj in article_entities:\n",
    "                \n",
    "                if e_obj['end'] > sentence[-1].idx: break\n",
    "                    \n",
    "                overlap_flag = False\n",
    "\n",
    "                for np_k, np_v in np_indices_dict.items(): \n",
    "                    overlap_flag = len(set(range(e_obj['begin'], e_obj['end'])).intersection(set(np_v))) > 0\n",
    "\n",
    "                    if overlap_flag: break\n",
    "\n",
    "                if overlap_flag: del np_indices_dict[np_k]\n",
    "        \n",
    "        for j, token in enumerate(sentence):\n",
    "            \n",
    "            spacy_annotation_obj[i]['tokens'].append({\n",
    "                'originalText': token.text,\n",
    "                'characterOffsetBegin': token.idx,\n",
    "                'characterOffsetEnd': token.idx + len(token.text),\n",
    "                'lemma': token.lemma_,\n",
    "                'sentence_index': i,\n",
    "                'index': j + 1,\n",
    "                'pos': token.pos_\n",
    "            })\n",
    "                \n",
    "            if np_flag: \n",
    "                for np_str, np_is in np_indices_dict.items(): \n",
    "                    if token.i in np_is: spacy_annotation_obj[i]['tokens'][j]['entity_id'] = ('np', np_str)\n",
    "                    \n",
    "            if token.dep_.lower() == 'root': spacy_dependencies_obj[i].append({\n",
    "                    'dep': token.dep_,\n",
    "                    'governor': 0,\n",
    "                    'governorGloss': 'ROOT',\n",
    "                    'dependent': token.i + 1,\n",
    "                    'dependentGloss': token.text,\n",
    "                })\n",
    "            else: spacy_dependencies_obj[i].append({\n",
    "                    'dep': token.dep_,\n",
    "                    'governor': token.head.i + 1,\n",
    "                    'governorGloss': token.head.text,\n",
    "                    'dependent': token.i + 1,\n",
    "                    'dependentGloss': token.text,\n",
    "                })    \n",
    "\n",
    "    for i, sentence in enumerate(spacy_document.sents):\n",
    "        for dep in spacy_dependencies_obj[i]: \n",
    "            gov_idx = sentence_aware_token_indices[dep['governor'] - 1] if dep['governor'] > 0 else -1\n",
    "            dep_idx = sentence_aware_token_indices[dep['dependent'] - 1]\n",
    "            dep_type = dep['dep'].lower()\n",
    "\n",
    "            if dep_type == 'root': spacy_annotation_obj[i]['dep_root'] = dep['dependent']\n",
    "            else:\n",
    "                if 'dependents' not in spacy_annotation_obj[i]['tokens'][gov_idx]: spacy_annotation_obj[i]['tokens'][gov_idx]['dependents'] = set()\n",
    "                spacy_annotation_obj[i]['tokens'][gov_idx]['dependents'].add((dep_idx, dep_type))\n",
    "\n",
    "            if 'governor' not in spacy_annotation_obj[i]['tokens'][dep_idx]: spacy_annotation_obj[i]['tokens'][dep_idx]['governor'] = set()\n",
    "            spacy_annotation_obj[i]['tokens'][dep_idx]['governor'].add((gov_idx, dep_type))\n",
    "            \n",
    "    for i, sentence in enumerate(spacy_document.sents):\n",
    "        for j, token in enumerate(spacy_annotation_obj[i]['tokens']):\n",
    "            token_range = set(range(token['characterOffsetBegin'], token['characterOffsetEnd']))\n",
    "            for entity in article_entities:\n",
    "                entity_range = set(range(entity['begin'], entity['end']))\n",
    "                if len(token_range.intersection(entity_range)) > 0: \n",
    "                    spacy_annotation_obj[i]['tokens'][j]['entity_id'] = ('dbpedia', entity['title'])\n",
    "                    \n",
    "    for i, sentence in enumerate(spacy_document.sents):\n",
    "        for j, token in enumerate(spacy_annotation_obj[i]['tokens']):\n",
    "            if 'governor' in spacy_annotation_obj[i]['tokens'][j]:\n",
    "                spacy_annotation_obj[i]['tokens'][j]['governor'] = list(spacy_annotation_obj[i]['tokens'][j]['governor'])\n",
    "            if 'dependents' in spacy_annotation_obj[i]['tokens'][j]:\n",
    "                spacy_annotation_obj[i]['tokens'][j]['dependents'] = list(spacy_annotation_obj[i]['tokens'][j]['dependents'])\n",
    "                 \n",
    "    for coref_cluster in get_coref_clusters(spacy_document, document_coreferences, article_entities):\n",
    "        main_title = get_coref_main(coref_cluster)\n",
    "        if not main_title: continue\n",
    "\n",
    "        for i, sentence in enumerate(spacy_document.sents):\n",
    "            for j, token in enumerate(spacy_annotation_obj[i]['tokens']):\n",
    "                token_range = set(range(token['characterOffsetBegin'], token['characterOffsetEnd']))\n",
    "                for c_chain in coref_cluster['chain']:\n",
    "                    c_range = set(range(c_chain['start'], c_chain['end']))\n",
    "                    if len(token_range.intersection(entity_range)) > 0: \n",
    "                        spacy_annotation_obj[i]['tokens'][j]['entity_id'] = ('dbpedia', main_title)\n",
    " \n",
    "    return spacy_annotation_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities, entity_extraction, sentiment_features\n",
    "\n",
    "def generate_article_entity_obj(spacy_annotation_obj):\n",
    "    entity_list = entity_extraction.EntityExtractor()\n",
    "    \n",
    "    for i, sentence in spacy_annotation_obj.items():\n",
    "\n",
    "        start_idx = curr_eid = None\n",
    "        for token in sentence['tokens']:\n",
    "\n",
    "            if 'entity_id' in token and curr_eid is None:\n",
    "                start_idx = token['index'] - 1\n",
    "                curr_eid = tuple(token['entity_id'])\n",
    "\n",
    "            if tuple(token.get('entity_id', ())) != curr_eid and curr_eid is not None:\n",
    "\n",
    "                end_idx = token['index'] - 1\n",
    "                raw_text = utilities.get_text(sentence['tokens'][start_idx:end_idx])\n",
    "\n",
    "                if curr_eid not in entity_list._occurances or raw_text not in entity_list._entity_to_id: \n",
    "                    if len(curr_eid) == 2 and 'trumpnote' in curr_eid[1]: \n",
    "                        print(curr_eid)\n",
    "                        print(token)\n",
    "                    entity_list.create_entity(('BLANK', raw_text), curr_eid, dangermode=True)\n",
    "\n",
    "                entity_list.add_occurance(('BLANK', raw_text), sentence['index'], start_idx, end_idx)\n",
    "                start_idx = curr_eid = None\n",
    "\n",
    "        if curr_eid is not None:\n",
    "\n",
    "            end_idx = token['index']\n",
    "            raw_text = utilities.get_text(sentence['tokens'][start_idx:end_idx])\n",
    "            entity_list.add_occurance(('BLANK', raw_text), sentence['index'], start_idx, end_idx)\n",
    "    \n",
    "    id_to_entity = {str(e[1][1]): e[0][1] for e in entity_list._entity_to_id.items()}\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_object(spacy_document, coreferences, article_entities):\n",
    "    \n",
    "    sentence_aware_token_indices = sentence_token_indices(spacy_document)\n",
    "\n",
    "    spacy_annotation_obj = transform_to_core_nlp_obj(\n",
    "        spacy_document,\n",
    "        coreferences,\n",
    "        sentence_aware_token_indices,\n",
    "        article_entities,\n",
    "        np_flag=False\n",
    "    )\n",
    "    \n",
    "    spacy_np_annotation_obj = transform_to_core_nlp_obj(\n",
    "        spacy_document,\n",
    "        coreferences,\n",
    "        sentence_aware_token_indices,\n",
    "        article_entities,\n",
    "        np_flag=True\n",
    "    )\n",
    "\n",
    "    entity_list = generate_article_entity_obj(spacy_annotation_obj)\n",
    "    entity_np_list = generate_article_entity_obj(spacy_np_annotation_obj)\n",
    "    \n",
    "    dependency_feature_list = sentiment_features.dependency_features(\n",
    "        [],\n",
    "        list(spacy_annotation_obj.values()),\n",
    "        entity_list\n",
    "    )\n",
    "    \n",
    "    dependency_np_feature_list = sentiment_features.dependency_features(\n",
    "        [],\n",
    "        list(spacy_np_annotation_obj.values()),\n",
    "        entity_np_list\n",
    "    )\n",
    "            \n",
    "    return (\n",
    "        spacy_annotation_obj,\n",
    "        spacy_np_annotation_obj,\n",
    "        entity_list,\n",
    "        entity_np_list,\n",
    "        dependency_feature_list,\n",
    "        dependency_np_feature_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def split_to_p_ids(s=3, n=140):\n",
    "    div = math.floor(n/s)\n",
    "    \n",
    "    p_list = [list(range(div))]\n",
    "        \n",
    "    for i in range(1, s): \n",
    "        p_max = max(p_list[i-1])\n",
    "        p_list.append([p_max + 1 + v for v in list(range(div))])\n",
    "        \n",
    "    return {\n",
    "        'p_ranges': [(min(p), max(p)) for p in p_list],\n",
    "        'p_ids': list(itertools.chain.from_iterable([[p[i] for p in p_list] for i in range(min([len(p) for p in p_list]))]))\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_balance = split_to_p_ids(5, len(spacy_paths))\n",
    "\n",
    "spacy_queue = list(zip(spacy_paths, load_balance['p_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_spacy_coref(path):\n",
    "    \n",
    "    p_id = path[1]\n",
    "    path = path[0]\n",
    "    \n",
    "    host = 'http://127.0.0.1:'\n",
    "    port = 8150\n",
    "    \n",
    "    for i, r in enumerate(load_balance['p_ranges']):\n",
    "        if p_id >= r[0] and p_id <= r[1]: \n",
    "            host += str(port + i) + '/coref'\n",
    "            break\n",
    "            \n",
    "    uid = path.split('/')[-1].replace('.pckl', '')\n",
    "    daily_folder = path.split('/')[-2]\n",
    "    \n",
    "    output_folder = output_dir + 'coreferences/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.pckl'\n",
    "    \n",
    "    if os.path.exists(output_file): return None\n",
    "    \n",
    "    spacy_article = load_spacy_obj(path, spacy_nlp.vocab)\n",
    "    \n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'wb') as f: pickle.dump({\n",
    "        'uid': uid,\n",
    "        'coref': get_coreferences(spacy_article.text, host=host)['coref']\n",
    "    }, f)  \n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing, re, os, json, pickle\n",
    "\n",
    "pool = Pool(5)\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(resolve_spacy_coref, spacy_queue),\n",
    "    desc='Article Coreference',\n",
    "    total=len(spacy_queue)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "\n",
    "def annotate_articles(path):\n",
    "\n",
    "    uid = path.split('/')[-1].replace('.pckl', '')\n",
    "    daily_folder = path.split('/')[-2]\n",
    "\n",
    "    if not os.path.exists(output_dir + 'entities/' + daily_folder + '/' + uid + '.json'): return None\n",
    "  \n",
    "    spacy_article = load_spacy_obj(path, spacy_nlp.vocab)\n",
    "    spacy_entities = load_entities(output_dir + 'entities/' + daily_folder + '/' + uid + '.json')['entities']\n",
    "    \n",
    "    spacy_entities = [e for e in spacy_entities if e]\n",
    "    \n",
    "    try:\n",
    "        with open(output_dir + 'coreferences/' + daily_folder + '/' + uid + '.pckl', 'rb') as f: spacy_corefs = pickle.load(f)\n",
    "    except Exception: \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    try: \n",
    "        spacy_annotation_obj, \\\n",
    "        spacy_np_annotation_obj, \\\n",
    "        entity_list, \\\n",
    "        entity_np_list, \\\n",
    "        dependency_feature_list, \\\n",
    "        dependency_np_feature_list = make_annotation_object(spacy_article, spacy_corefs, spacy_entities)\n",
    "    except Exception as ex: \n",
    "        print(ex)\n",
    "        print(path)\n",
    "        print()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    output_folder = output_dir + 'annotations/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: json.dump({\n",
    "        'uid': uid,\n",
    "        'annotations': spacy_annotation_obj\n",
    "    }, f)\n",
    "\n",
    "    output_folder = output_dir + 'np_annotations/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: json.dump({\n",
    "        'uid': uid,\n",
    "        'annotations': spacy_np_annotation_obj\n",
    "    }, f)\n",
    "\n",
    "    output_folder = output_dir + 'annotated_entities/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: json.dump({\n",
    "        'uid': uid,\n",
    "        'entities': jsonpickle.encode(entity_list)\n",
    "    }, f)\n",
    "\n",
    "    output_folder = output_dir + 'np_annotated_entities/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: json.dump({\n",
    "        'uid': uid,\n",
    "        'entities': jsonpickle.encode(entity_np_list)\n",
    "    }, f)            \n",
    "\n",
    "    output_folder = output_dir + 'dependency_features/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.pckl'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'wb') as f: pickle.dump({\n",
    "        'uid': uid,\n",
    "        'dependency_features': dependency_feature_list\n",
    "    }, f)    \n",
    "\n",
    "    output_folder = output_dir + 'np_dependency_features/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.pckl'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'wb') as f: pickle.dump({\n",
    "        'uid': uid,\n",
    "        'dependency_features': dependency_np_feature_list\n",
    "    }, f)  \n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e65167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "\n",
    "def load_entities(path):\n",
    "    with open(path, 'r') as f: entity_list = json.load(f)\n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4941c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de714daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing, re, os, json, pickle\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count() - 8)\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(annotate_articles, spacy_paths),\n",
    "    desc='Article Dependency Annotation',\n",
    "    total=len(spacy_paths)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a37cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import DepDirection, find_dep_path\n",
    "import sentiment_features, itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_pair_annotations(path):\n",
    "\n",
    "    daily_folder = path.split('/')[-2]\n",
    "\n",
    "    spacy_article = load_spacy_obj(path, spacy_nlp.vocab)\n",
    "   \n",
    "    path = path.replace('/spacy/', '/{}/').replace('.pckl', '{}')\n",
    "\n",
    "    if not os.path.exists(path.format('annotations', '.json')): return None\n",
    "    if os.path.exists(path.format('pair_indices', '.json')): return None\n",
    "\n",
    "    with open(path.format('annotations', '.json'), 'r') as f: spacy_annotation_obj = json.load(f)\n",
    "        \n",
    "    uid = spacy_annotation_obj['uid']\n",
    "    spacy_annotation_obj = spacy_annotation_obj['annotations']\n",
    "\n",
    "    entity_sentence_indices = defaultdict(lambda: [])\n",
    "\n",
    "    for sentence_i, annotations in spacy_annotation_obj.items():\n",
    "        for t in annotations['tokens']:\n",
    "            if 'entity_id' in t and t['entity_id'][0] == 'dbpedia': \n",
    "                if sentence_i in entity_sentence_indices[t['entity_id'][1]]: continue\n",
    "                entity_sentence_indices[t['entity_id'][1]].append(sentence_i)\n",
    "\n",
    "    with open(path.format('annotated_entities', '.json'), 'r') as f: entity_list = json.load(f)\n",
    "    entity_list = jsonpickle.decode(entity_list['entities'])\n",
    "                    \n",
    "    pair_sentence_indices = defaultdict(lambda: [])\n",
    "\n",
    "    for sentence_i, annotations in spacy_annotation_obj.items():\n",
    "        entity_list = [t['entity_id'][1] for t in annotations['tokens'] if 'entity_id' in t and t['entity_id'][0] == 'dbpedia']\n",
    "        for pair in itertools.combinations(entity_list, 2):\n",
    "            if pair[0] == pair[1]: continue\n",
    "            _pair = [pair[0], pair[1]]\n",
    "            _pair.sort()\n",
    "\n",
    "            pair = (_pair[0], _pair[1])\n",
    "\n",
    "            if sentence_i in pair_sentence_indices[pair]: continue\n",
    "            pair_sentence_indices[pair].append(sentence_i)\n",
    "\n",
    "    entity_sentence_indices, pair_sentence_indices = dict(entity_sentence_indices), dict(pair_sentence_indices)\n",
    "\n",
    "    output_folder = output_dir + 'entity_indices/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'indices': jsonpickle.encode(entity_sentence_indices)\n",
    "    }))\n",
    "\n",
    "    output_folder = output_dir + 'pair_indices/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'indices': jsonpickle.encode(pair_sentence_indices)\n",
    "    }))\n",
    "\n",
    "    entity_annotation_dict = defaultdict(lambda: [])\n",
    "    entity_sentence_dict = defaultdict(lambda: [])\n",
    "    entity_pair_annotation_dict = defaultdict(lambda: [])\n",
    "    entity_pair_sentence_dict = defaultdict(lambda: [])\n",
    "\n",
    "    article_sents = list(spacy_article.sents)\n",
    "\n",
    "    for entity in entity_sentence_indices:\n",
    "\n",
    "        for sentence_i in entity_sentence_indices[entity]:\n",
    "            \n",
    "            #######################################################################\n",
    "            # entity_sentence_dict[entity].append(article_sents[int(sentence_i)]) #\n",
    "            #######################################################################\n",
    "\n",
    "            entity_annotation_dict[entity].append(spacy_annotation_obj[sentence_i])\n",
    "\n",
    "    for pair in pair_sentence_indices:\n",
    "\n",
    "        for sentence_i in pair_sentence_indices[pair]:\n",
    "            \n",
    "            ##########################################################################\n",
    "            # entity_pair_sentence_dict[pair].append(article_sents[int(sentence_i)]) #\n",
    "            ##########################################################################\n",
    "\n",
    "            entity_pair_annotation_dict[pair].append(spacy_annotation_obj[sentence_i])\n",
    "\n",
    "    entity_annotation_dict = dict(entity_annotation_dict)\n",
    "    entity_pair_annotation_dict = dict(entity_pair_annotation_dict)\n",
    "\n",
    "    ###############################################################\n",
    "    # entity_pair_sentence_dict = dict(entity_pair_sentence_dict) #\n",
    "    # entity_sentence_dict = dict(entity_sentence_dict)           #\n",
    "    ###############################################################\n",
    "\n",
    "    output_folder = output_dir + 'entity_annotations/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'indices': jsonpickle.encode(entity_annotation_dict)\n",
    "    }))\n",
    "\n",
    "    output_folder = output_dir + 'pair_annotations/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'indices': jsonpickle.encode(entity_pair_annotation_dict)\n",
    "    }))\n",
    "\n",
    "    pair_annotation_dependency_features = defaultdict(lambda: [])\n",
    "\n",
    "    for pair in list(entity_pair_annotation_dict.keys()):\n",
    "\n",
    "        np_dep_path_feature = []\n",
    "\n",
    "        for annotation in entity_pair_annotation_dict[pair]:\n",
    "\n",
    "            tokens = annotation['tokens']\n",
    "            source_indices = [i for i, t in enumerate(tokens) if 'entity_id' in t and t['entity_id'][1] == pair[0]]\n",
    "            destination_indices = [i for i, t in enumerate(tokens) if 'entity_id' in t and t['entity_id'][1] == pair[1]]\n",
    "\n",
    "            for source_destination in itertools.product(source_indices, destination_indices):\n",
    "\n",
    "                dep_path = find_dep_path(tokens, source_destination[0], source_destination[1])\n",
    "\n",
    "                dep_path = [\n",
    "                    (( DepDirection.DEP if dep_dir == DepDirection.GOV else DepDirection.GOV, dep_type), dep_idx) \n",
    "                    for (dep_dir, dep_type), dep_idx in dep_path\n",
    "                ]\n",
    "\n",
    "                dep_path_features = sentiment_features.dep_path_features([], tokens, dep_path)\n",
    "\n",
    "                if len(dep_path_features) > 0: np_dep_path_feature += [dpf[1] for dpf in dep_path_features]\n",
    "\n",
    "                dep_path = find_dep_path(tokens, source_destination[1], source_destination[0])\n",
    "\n",
    "                dep_path = [\n",
    "                    (( DepDirection.DEP if dep_dir == DepDirection.GOV else DepDirection.GOV, dep_type), dep_idx) \n",
    "                    for (dep_dir, dep_type), dep_idx in dep_path\n",
    "                ]\n",
    "\n",
    "                dep_path_features = sentiment_features.dep_path_features([], tokens, dep_path)\n",
    "\n",
    "                if len(dep_path_features) > 0: np_dep_path_feature += [dpf[1] for dpf in dep_path_features]\n",
    "\n",
    "        pair_annotation_dependency_features[pair] = np_dep_path_feature\n",
    "\n",
    "    pair_annotation_dependency_features = dict(pair_annotation_dependency_features)\n",
    "\n",
    "    output_folder = output_dir + 'pair_dependency_features/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(output_file, 'w') as f: f.write(json.dumps({\n",
    "        'uid': uid,\n",
    "        'dependency_features': jsonpickle.encode(pair_annotation_dependency_features)\n",
    "    }))\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, jsonpickle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count() - 8)\n",
    "\n",
    "for i in tqdm(\n",
    "    pool.imap_unordered(\n",
    "        extract_pair_annotations,\n",
    "        spacy_paths\n",
    "    ),\n",
    "    desc='Entity Pair Extraction',\n",
    "    total=len(spacy_paths)\n",
    "): pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "\n",
    "def load_gzip(path, func=json.loads):\n",
    "    with gzip.open(path, 'r') as f: data = func(f.read().decode('utf-8'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e713719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pickle, gzip, gc\n",
    "\n",
    "def export_entity_np_pairs(path):\n",
    "    \n",
    "    uid = path.split('/')[-1].replace('.pckl', '')\n",
    "    daily_folder = path.split('/')[-2]\n",
    "    \n",
    "    _path = path.replace('/spacy/', '/{}/').replace('.pckl', '{}')\n",
    "\n",
    "    if not os.path.exists(_path.format('entity_indices', '.json')): return None\n",
    "    if os.path.exists(output_dir + 'entity_np_sentences/' + daily_folder + '/' + uid + '.json') and \\\n",
    "       os.path.exists(output_dir + 'entity_np_annotations/' + daily_folder + '/' + uid + '.json'): return None\n",
    "\n",
    "    spacy_doc = load_spacy_obj(path, spacy_nlp.vocab)\n",
    "    path = _path\n",
    "    \n",
    "    with open(path.format('entity_indices', '.json'), 'r') as f: entity_sentence_indices = json.load(f)\n",
    "    entity_sentence_indices['indices'] = jsonpickle.decode(entity_sentence_indices['indices'])\n",
    "    entity_sentence_indices = entity_sentence_indices['indices']\n",
    "\n",
    "    with open(path.format('np_annotations', '.json'), 'r') as f: np_annotation_objects = json.load(f)\n",
    "    np_annotation_objects = np_annotation_objects['annotations']\n",
    "\n",
    "    entity_np_occuring_sentences = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    \n",
    "    entity_np_occuring_annotations = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "    article_sents = list(spacy_doc.sents)\n",
    "\n",
    "    for entity in entity_sentence_indices:\n",
    "\n",
    "        for sentence_i in entity_sentence_indices[entity]:\n",
    "            sentence = article_sents[int(sentence_i)]\n",
    "\n",
    "            for np in sentence.noun_chunks: \n",
    "                \n",
    "                entity_np_occuring_sentences[entity][np.text].append(sentence.text)\n",
    "                        \n",
    "                entity_np_occuring_annotations[entity][np.text].append(np_annotation_objects[str(sentence_i)])\n",
    "    \n",
    "    for k1 in entity_np_occuring_sentences:\n",
    "        for k2 in entity_np_occuring_sentences[k1]:\n",
    "            entity_np_occuring_sentences[k1] = dict(entity_np_occuring_sentences[k1])\n",
    "\n",
    "    entity_np_occuring_sentences = dict(entity_np_occuring_sentences)\n",
    "    \n",
    "    for k1 in entity_np_occuring_annotations:\n",
    "        for k2 in entity_np_occuring_annotations[k1]:\n",
    "            entity_np_occuring_annotations[k1] = dict(entity_np_occuring_annotations[k1])\n",
    "\n",
    "    entity_np_occuring_annotations = dict(entity_np_occuring_annotations)\n",
    "\n",
    "    output_folder = output_dir + 'entity_np_sentences/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "            \n",
    "    with gzip.open(output_file, 'wt', encoding='UTF-8') as f: json.dump({\n",
    "        'uid': uid,\n",
    "        'entity_np_sentences': entity_np_occuring_sentences\n",
    "    }, f)\n",
    "    \n",
    "    output_folder = output_dir + 'entity_np_annotations/' + daily_folder + '/'\n",
    "    output_file = output_folder + uid + '.json'\n",
    "    \n",
    "    if not os.path.exists(output_folder): os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    with gzip.open(output_file, 'wt', encoding='UTF-8') as f: json.dump({\n",
    "        'uid': uid,\n",
    "        'entity_np_annotations': entity_np_occuring_annotations\n",
    "    }, f)\n",
    "    \n",
    "    entity_np_occuring_annotations = None\n",
    "    entity_np_occuring_sentences = None\n",
    "    spacy_doc = None\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json, jsonpickle, multiprocessing, os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(multiprocessing.cpu_count() - 16)\n",
    "\n",
    "for d in tqdm(\n",
    "    pool.imap_unordered(export_entity_np_pairs, spacy_paths),\n",
    "    desc='Export Entity-NP',\n",
    "    total=len(spacy_paths)\n",
    "): del d\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
